{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyPXiYbBH0uGiXkZdj2oOaQk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SomeiLam/api-docs-ai-agent-colab/blob/main/sikka_apis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Summary\n",
        "\n",
        "This Colab notebook implements a full‑stack, retrieval‑augmented code generation pipeline for building payment integration apps using Sikka APIs. It breaks the user’s high‑level request into discrete AI agents that each handle one piece of the workflow:\n",
        "\n",
        "1. **Query Optimizer**  \n",
        "   Normalizes and scopes the user’s message, detects tech stacks, and produces structured prompts.  \n",
        "2. **API‑Docs Generator**  \n",
        "   Retrieves relevant API reference snippets via FAISS and generates detailed endpoint documentation (base URL, version, headers, parameters, sample requests/responses).  \n",
        "3. **Frontend Code Generator**  \n",
        "   Produces React (or Next.js) components for the checkout UI, wiring up form state, loading, and error handling.  \n",
        "4. **Backend Code Generator**  \n",
        "   Generates Node.js + Express routes that obtain/refresh the Sikka `request_key`, save cards, and process payments.  \n",
        "5. **Code Evaluator & Refinement**  \n",
        "   Splits evaluation into frontend/backend reviewers, collects issues and suggestions, and applies fixes via a reusable refinement agent.  \n",
        "6. **Documentation Formatter**  \n",
        "   Assembles the optimized query, tech stack, API docs, frontend code, and backend code into a polished Markdown deliverable.  \n",
        "7. **Follow‑up QA**  \n",
        "   Maintains conversational memory and combines the final document with FAISS‑retrieved context to answer user follow‑up questions without re‑running the full pipeline.\n",
        "\n",
        "Together, these agents demonstrate how to orchestrate multiple LLM calls in Colab—leveraging CrewAI, FAISS, and OpenAI’s GPT models—to automate everything from requirements analysis to production‑ready code and documentation.\n",
        "\n",
        "---\n",
        "\n",
        "### Notebook Installation\n",
        "\n",
        "Before running any cells, install and pin your dependencies for a reproducible environment. In a new Colab cell, run\n",
        "```\n",
        "# Install core libraries for FAISS vector search, OpenAI API access, tokenization, agent orchestration, and retrieval workflows\n",
        "!pip install --upgrade --no-cache-dir \\\n",
        "    faiss-cpu    # vector similarity search engine for nearest‑neighbor lookup  \n",
        "    openai       # official OpenAI Python client for embeddings and chat completions  \n",
        "    tiktoken     # high‑speed tokenizer compatible with OpenAI models  \n",
        "    langchain    # framework for building retrieval‑augmented LLM applications  \n",
        "    crewai       # library for coordinating multi‑agent AI workflows  \n",
        "```"
      ],
      "metadata": {
        "id": "7OohWzNbognk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0u3OYvvALXrY",
        "outputId": "30a6b506-00f7-43c9-b09f-b6a5a4bce662"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.75.0)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.23)\n",
            "Collecting crewai\n",
            "  Downloading crewai-0.114.0-py3-none-any.whl.metadata (33 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.3)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.51 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.52)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.31)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Collecting appdirs>=1.4.4 (from crewai)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting auth0-python>=4.7.1 (from crewai)\n",
            "  Downloading auth0_python-4.9.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from crewai) (1.9.0)\n",
            "Collecting chromadb>=0.5.23 (from crewai)\n",
            "  Downloading chromadb-1.0.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: click>=8.1.7 in /usr/local/lib/python3.11/dist-packages (from crewai) (8.1.8)\n",
            "Collecting instructor>=1.3.3 (from crewai)\n",
            "  Downloading instructor-1.7.9-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting json-repair>=0.25.2 (from crewai)\n",
            "  Downloading json_repair-0.41.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting json5>=0.10.0 (from crewai)\n",
            "  Downloading json5-0.12.0-py3-none-any.whl.metadata (36 kB)\n",
            "Collecting jsonref>=1.1.0 (from crewai)\n",
            "  Downloading jsonref-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting litellm==1.60.2 (from crewai)\n",
            "  Downloading litellm-1.60.2-py3-none-any.whl.metadata (36 kB)\n",
            "Requirement already satisfied: openpyxl>=3.1.5 in /usr/local/lib/python3.11/dist-packages (from crewai) (3.1.5)\n",
            "Requirement already satisfied: opentelemetry-api>=1.30.0 in /usr/local/lib/python3.11/dist-packages (from crewai) (1.32.1)\n",
            "Collecting opentelemetry-exporter-otlp-proto-http>=1.30.0 (from crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_http-1.32.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.30.0 in /usr/local/lib/python3.11/dist-packages (from crewai) (1.32.1)\n",
            "Collecting pdfplumber>=0.11.4 (from crewai)\n",
            "  Downloading pdfplumber-0.11.6-py3-none-any.whl.metadata (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 kB\u001b[0m \u001b[31m121.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=1.0.0 (from crewai)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting pyvis>=0.3.2 (from crewai)\n",
            "  Downloading pyvis-0.3.2-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting tomli-w>=1.1.0 (from crewai)\n",
            "  Downloading tomli_w-1.2.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting tomli>=2.0.2 (from crewai)\n",
            "  Downloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting uv>=0.4.25 (from crewai)\n",
            "  Downloading uv-0.6.14-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from litellm==1.60.2->crewai) (3.11.15)\n",
            "Collecting httpx<1,>=0.23.0 (from openai)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.11/dist-packages (from litellm==1.60.2->crewai) (8.6.1)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from litellm==1.60.2->crewai) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from litellm==1.60.2->crewai) (4.23.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from litellm==1.60.2->crewai) (0.21.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: cryptography>=43.0.1 in /usr/local/lib/python3.11/dist-packages (from auth0-python>=4.7.1->crewai) (43.0.3)\n",
            "Requirement already satisfied: pyjwt>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from auth0-python>=4.7.1->crewai) (2.10.1)\n",
            "Requirement already satisfied: urllib3>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from auth0-python>=4.7.1->crewai) (2.3.0)\n",
            "Collecting build>=1.0.3 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting chroma-hnswlib==0.7.6 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (252 bytes)\n",
            "Collecting fastapi==0.115.9 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading fastapi-0.115.9-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting uvicorn>=0.18.3 (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai)\n",
            "  Downloading uvicorn-0.34.1-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting posthog>=2.4.0 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading posthog-3.25.0-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting onnxruntime>=1.14.1 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting opentelemetry-exporter-otlp-proto-grpc>=1.2.0 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting opentelemetry-instrumentation-fastapi>=0.41b0 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl.metadata (2.2 kB)\n",
            "Collecting pypika>=0.48.9 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading PyPika-0.48.9.tar.gz (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m187.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting overrides>=7.3.1 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai) (1.71.0)\n",
            "Collecting bcrypt>=4.0.1 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai) (0.15.2)\n",
            "Collecting kubernetes>=28.1.0 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading kubernetes-32.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai) (9.1.2)\n",
            "Collecting mmh3>=4.0.1 (from chromadb>=0.5.23->crewai)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai) (3.10.16)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb>=0.5.23->crewai) (13.9.4)\n",
            "Collecting starlette<0.46.0,>=0.40.0 (from fastapi==0.115.9->chromadb>=0.5.23->crewai)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.8)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: docstring-parser<1.0,>=0.16 in /usr/local/lib/python3.11/dist-packages (from instructor>=1.3.3->crewai) (0.16)\n",
            "Collecting jiter<1,>=0.4.0 (from openai)\n",
            "  Downloading jiter-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.0 in /usr/local/lib/python3.11/dist-packages (from instructor>=1.3.3->crewai) (2.33.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.51->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl>=3.1.5->crewai) (2.0.0)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.30.0->crewai) (1.2.18)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-http>=1.30.0->crewai) (1.70.0)\n",
            "Collecting opentelemetry-exporter-otlp-proto-common==1.32.1 (from opentelemetry-exporter-otlp-proto-http>=1.30.0->crewai)\n",
            "  Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting opentelemetry-proto==1.32.1 (from opentelemetry-exporter-otlp-proto-http>=1.30.0->crewai)\n",
            "  Downloading opentelemetry_proto-1.32.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: protobuf<6.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-proto==1.32.1->opentelemetry-exporter-otlp-proto-http>=1.30.0->crewai) (5.29.4)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.53b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.30.0->crewai) (0.53b1)\n",
            "Collecting pdfminer.six==20250327 (from pdfplumber>=0.11.4->crewai)\n",
            "  Downloading pdfminer_six-20250327-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber>=0.11.4->crewai) (11.1.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber>=0.11.4->crewai)\n",
            "  Downloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m212.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20250327->pdfplumber>=0.11.4->crewai) (3.4.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n",
            "Requirement already satisfied: ipython>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from pyvis>=0.3.2->crewai) (7.34.0)\n",
            "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from pyvis>=0.3.2->crewai) (4.0.5)\n",
            "Requirement already satisfied: networkx>=1.11 in /usr/local/lib/python3.11/dist-packages (from pyvis>=0.3.2->crewai) (3.4.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.60.2->crewai) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.60.2->crewai) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.60.2->crewai) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.60.2->crewai) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.60.2->crewai) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.60.2->crewai) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->litellm==1.60.2->crewai) (1.19.0)\n",
            "Collecting pyproject_hooks (from build>=1.0.3->chromadb>=0.5.23->crewai)\n",
            "  Downloading pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=43.0.1->auth0-python>=4.7.1->crewai) (1.17.1)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api>=1.30.0->crewai) (1.17.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata>=6.8.0->litellm==1.60.2->crewai) (3.21.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (75.2.0)\n",
            "Collecting jedi>=0.16 (from ipython>=5.3.0->pyvis>=0.3.2->crewai)\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (3.0.51)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from ipython>=5.3.0->pyvis>=0.3.2->crewai) (4.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm==1.60.2->crewai) (3.0.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.51->langchain) (3.0.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.60.2->crewai) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.60.2->crewai) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<5.0.0,>=4.22.0->litellm==1.60.2->crewai) (0.24.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (3.2.2)\n",
            "Collecting durationpy>=0.7 (from kubernetes>=28.1.0->chromadb>=0.5.23->crewai)\n",
            "  Downloading durationpy-0.9-py3-none-any.whl.metadata (338 bytes)\n",
            "Collecting coloredlogs (from onnxruntime>=1.14.1->chromadb>=0.5.23->crewai)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.5.23->crewai) (25.2.10)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb>=0.5.23->crewai) (1.13.1)\n",
            "Collecting opentelemetry-instrumentation-asgi==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.23->crewai)\n",
            "  Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting opentelemetry-instrumentation==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.23->crewai)\n",
            "  Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting opentelemetry-util-http==0.53b1 (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.23->crewai)\n",
            "  Downloading opentelemetry_util_http-0.53b1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting asgiref~=3.0 (from opentelemetry-instrumentation-asgi==0.53b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb>=0.5.23->crewai)\n",
            "  Downloading asgiref-3.8.1-py3-none-any.whl.metadata (9.3 kB)\n",
            "Collecting monotonic>=1.5 (from posthog>=2.4.0->chromadb>=0.5.23->crewai)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog>=2.4.0->chromadb>=0.5.23->crewai)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb>=0.5.23->crewai) (3.0.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers->litellm==1.60.2->crewai) (0.30.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb>=0.5.23->crewai) (1.5.4)\n",
            "Collecting httptools>=0.6.3 (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai)\n",
            "  Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai)\n",
            "  Downloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Collecting watchfiles>=0.13 (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai)\n",
            "  Downloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb>=0.5.23->crewai) (15.0.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=43.0.1->auth0-python>=4.7.1->crewai) (2.22)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm==1.60.2->crewai) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->litellm==1.60.2->crewai) (2025.3.2)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.8.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb>=0.5.23->crewai) (0.1.2)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=5.3.0->pyvis>=0.3.2->crewai) (0.2.13)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.14.1->chromadb>=0.5.23->crewai)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb>=0.5.23->crewai) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb>=0.5.23->crewai) (0.6.1)\n",
            "Downloading faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m252.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m326.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading crewai-0.114.0-py3-none-any.whl (285 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m285.5/285.5 kB\u001b[0m \u001b[31m249.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading litellm-1.60.2-py3-none-any.whl (6.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m240.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading auth0_python-4.9.0-py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 kB\u001b[0m \u001b[31m262.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chromadb-1.0.5-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m223.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chroma_hnswlib-0.7.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m331.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastapi-0.115.9-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m246.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m222.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading instructor-1.7.9-py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m267.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.8.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.6/345.6 kB\u001b[0m \u001b[31m185.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading json_repair-0.41.1-py3-none-any.whl (21 kB)\n",
            "Downloading json5-0.12.0-py3-none-any.whl (36 kB)\n",
            "Downloading jsonref-1.1.0-py3-none-any.whl (9.4 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_http-1.32.1-py3-none-any.whl (17 kB)\n",
            "Downloading opentelemetry_exporter_otlp_proto_common-1.32.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_proto-1.32.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m191.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfplumber-0.11.6-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.2/60.2 kB\u001b[0m \u001b[31m258.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20250327-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m254.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading pyvis-0.3.2-py3-none-any.whl (756 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m323.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli-2.2.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (236 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.0/236.0 kB\u001b[0m \u001b[31m306.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli_w-1.2.0-py3-none-any.whl (6.7 kB)\n",
            "Downloading uv-0.6.14-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.9/16.9 MB\u001b[0m \u001b[31m243.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bcrypt-4.3.0-cp39-abi3-manylinux_2_34_x86_64.whl (284 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.2/284.2 kB\u001b[0m \u001b[31m249.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
            "Downloading kubernetes-32.0.1-py2.py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m332.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m264.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m198.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opentelemetry_exporter_otlp_proto_grpc-1.32.1-py3-none-any.whl (18 kB)\n",
            "Downloading opentelemetry_instrumentation_fastapi-0.53b1-py3-none-any.whl (12 kB)\n",
            "Downloading opentelemetry_instrumentation-0.53b1-py3-none-any.whl (30 kB)\n",
            "Downloading opentelemetry_instrumentation_asgi-0.53b1-py3-none-any.whl (16 kB)\n",
            "Downloading opentelemetry_util_http-0.53b1-py3-none-any.whl (7.3 kB)\n",
            "Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Downloading posthog-3.25.0-py2.py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.1/89.1 kB\u001b[0m \u001b[31m255.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-4.30.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m270.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvicorn-0.34.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m221.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading durationpy-0.9-py3-none-any.whl (3.5 kB)\n",
            "Downloading httptools-0.6.4-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (459 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m304.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m323.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m244.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading uvloop-0.21.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m290.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading watchfiles-1.0.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m323.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m206.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
            "Downloading asgiref-3.8.1-py3-none-any.whl (23 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m218.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pypika\n",
            "  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pypika: filename=pypika-0.48.9-py2.py3-none-any.whl size=53800 sha256=396a5ca64207c76262f428cbf8359b210b0e20bf31fbd0caadaea931080b56fc\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ro3b2ek1/wheels/a3/01/bd/4c40ceb9d5354160cb186dcc153360f4ab7eb23e2b24daf96d\n",
            "Successfully built pypika\n",
            "Installing collected packages: pypika, monotonic, durationpy, appdirs, uvloop, uvicorn, uv, tomli-w, tomli, python-dotenv, pyproject_hooks, pypdfium2, overrides, opentelemetry-util-http, opentelemetry-proto, mmh3, jsonref, json5, json-repair, jiter, jedi, humanfriendly, httptools, faiss-cpu, chroma-hnswlib, bcrypt, backoff, asgiref, watchfiles, tiktoken, starlette, posthog, opentelemetry-exporter-otlp-proto-common, httpx, coloredlogs, build, pyvis, pdfminer.six, onnxruntime, kubernetes, fastapi, auth0-python, pdfplumber, opentelemetry-instrumentation, litellm, instructor, opentelemetry-instrumentation-asgi, opentelemetry-exporter-otlp-proto-http, opentelemetry-exporter-otlp-proto-grpc, opentelemetry-instrumentation-fastapi, chromadb, crewai\n",
            "  Attempting uninstall: jiter\n",
            "    Found existing installation: jiter 0.9.0\n",
            "    Uninstalling jiter-0.9.0:\n",
            "      Successfully uninstalled jiter-0.9.0\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-genai 1.10.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed appdirs-1.4.4 asgiref-3.8.1 auth0-python-4.9.0 backoff-2.2.1 bcrypt-4.3.0 build-1.2.2.post1 chroma-hnswlib-0.7.6 chromadb-1.0.5 coloredlogs-15.0.1 crewai-0.114.0 durationpy-0.9 faiss-cpu-1.10.0 fastapi-0.115.9 httptools-0.6.4 httpx-0.27.2 humanfriendly-10.0 instructor-1.7.9 jedi-0.19.2 jiter-0.8.2 json-repair-0.41.1 json5-0.12.0 jsonref-1.1.0 kubernetes-32.0.1 litellm-1.60.2 mmh3-5.1.0 monotonic-1.6 onnxruntime-1.21.1 opentelemetry-exporter-otlp-proto-common-1.32.1 opentelemetry-exporter-otlp-proto-grpc-1.32.1 opentelemetry-exporter-otlp-proto-http-1.32.1 opentelemetry-instrumentation-0.53b1 opentelemetry-instrumentation-asgi-0.53b1 opentelemetry-instrumentation-fastapi-0.53b1 opentelemetry-proto-1.32.1 opentelemetry-util-http-0.53b1 overrides-7.7.0 pdfminer.six-20250327 pdfplumber-0.11.6 posthog-3.25.0 pypdfium2-4.30.1 pypika-0.48.9 pyproject_hooks-1.2.0 python-dotenv-1.1.0 pyvis-0.3.2 starlette-0.45.3 tiktoken-0.9.0 tomli-2.2.1 tomli-w-1.2.0 uv-0.6.14 uvicorn-0.34.1 uvloop-0.21.0 watchfiles-1.0.5\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --no-cache-dir faiss-cpu openai tiktoken langchain crewai\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block Summary**\n",
        "\n",
        "This initialization cell prepares our Colab environment by:\n",
        "\n",
        "- **Importing standard libraries**  \n",
        "  - `os`, `pathlib` for file and environment management  \n",
        "  - `json`, `pickle` for data serialization  \n",
        "  - `textwrap` for text formatting  \n",
        "  - `getpass` for optional secure input  \n",
        "  - `typing` helpers (`List`, `Dict`) for clearer function signatures  \n",
        "\n",
        "- **Importing core third‑party packages**  \n",
        "  - `numpy` for numerical arrays  \n",
        "  - `faiss` for fast similarity search over embeddings  \n",
        "  - `tiktoken` for tokenizing inputs to OpenAI models  \n",
        "  - `openai` for interacting with OpenAI’s embeddings and chat APIs  \n",
        "\n",
        "- **Loading your OpenAI API key securely**  \n",
        "  - Go to the Colab toolbar, click the **lock icon (Secrets pane)** on the left, choose **“Add secret”**, enter **`OPENAI_API_KEY`** as the name and paste your key as the value.  \n",
        "  - This makes it available via `from google.colab import userdata` and `userdata.get(\"OPENAI_API_KEY\")`.  \n",
        "\n",
        "- **Setting the embedding model**  \n",
        "  - `EMBED_MODEL = \"text-embedding-3-small\"` will be used for all vectorization calls.  \n",
        "\n",
        "With these steps, the notebook can safely call the OpenAI and FAISS-based vector search functions without exposing your secret in plain code.\n"
      ],
      "metadata": {
        "id": "SgK9kXoBuhsB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import pathlib\n",
        "import textwrap\n",
        "import pickle\n",
        "import getpass\n",
        "from typing import List, Dict\n",
        "\n",
        "import numpy as np\n",
        "import faiss\n",
        "import tiktoken\n",
        "import openai\n",
        "\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load the OpenAI API key from Colab secrets and set it as an environment variable\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "\n",
        "# Name of the embedding model to use for vectorization\n",
        "EMBED_MODEL = \"text-embedding-3-small\"\n"
      ],
      "metadata": {
        "id": "_I9m4P5vLc6E"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block Summary**\n",
        "\n",
        "This cell defines and runs a **`collect_docs`** function to extract structured documentation from the Postman collection JSON:\n",
        "\n",
        "1. **Loading & Parsing**  \n",
        "   - Reads `sikka-apis.json` into `root`.\n",
        "\n",
        "2. **Depth‑First Traversal**  \n",
        "   - Uses a stack to walk through nested dicts and lists.\n",
        "   - Tracks the “path” (e.g. `Sikka API v4 › Authorization › Generate request_key`) to locate each item.\n",
        "\n",
        "3. **Extracting Snippets**  \n",
        "   - Gathers:\n",
        "     - Plain-text `description` fields.\n",
        "     - Request-level docs & raw JSON bodies.\n",
        "     - Sample responses (`response.body`).\n",
        "     - Inline scripts (`event → script.exec`).\n",
        "\n",
        "4. **Output Format**  \n",
        "   - Returns a list of dicts, each with:\n",
        "     - `\"content\"`: a markdown string combining the snippets.\n",
        "     - `\"path\"`: the hierarchical breadcrumb.\n",
        "\n",
        "5. **Role in Pipeline**  \n",
        "   - These extracted docs become the **knowledge base** for FAISS indexing and subsequent retrieval‑augmented LLM prompts.\n"
      ],
      "metadata": {
        "id": "vS7BYu4pvRy5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and parse the Postman collection JSON file\n",
        "FILE = pathlib.Path(\"sikka-apis.json\")   # Path to the raw API spec\n",
        "root = json.loads(FILE.read_text())      # Parse JSON into Python object\n",
        "\n",
        "def collect_docs(node):\n",
        "    \"\"\"\n",
        "    Traverse a Postman collection (dicts & lists) depth‑first, extracting:\n",
        "      - Resource & request descriptions\n",
        "      - Endpoint (METHOD + full URL)\n",
        "      - Headers\n",
        "      - Request body\n",
        "      - Response samples\n",
        "      - Example scripts\n",
        "\n",
        "    Returns a list of {\"content\": markdown, \"path\": hierarchical name}.\n",
        "    \"\"\"\n",
        "    stack, out = [([], node)], []\n",
        "\n",
        "    while stack:\n",
        "        path, cur = stack.pop()\n",
        "\n",
        "        if isinstance(cur, dict):\n",
        "            name     = cur.get(\"name\") or \"<no‑name>\"\n",
        "            new_path = path + [name]\n",
        "            buckets  = []\n",
        "\n",
        "            # 1) Resource‑level description\n",
        "            if desc := cur.get(\"description\"):\n",
        "                buckets.append(\"**Description:**\\n\" + desc.strip())\n",
        "\n",
        "            # 2) Request block\n",
        "            req    = cur.get(\"request\", {}) or {}\n",
        "            method = req.get(\"method\", \"\").upper()\n",
        "\n",
        "            # Build the full URL (either raw or protocol + host + path)\n",
        "            raw_url = req.get(\"url\", {}).get(\"raw\")\n",
        "            if not raw_url and isinstance(req.get(\"url\"), dict):\n",
        "                u = req[\"url\"]\n",
        "                host = \".\".join(u.get(\"host\", []))\n",
        "                path = \"/\".join(u.get(\"path\", []))\n",
        "                raw_url = f\"{u.get('protocol','https')}://{host}/{path}\"\n",
        "\n",
        "            # 2a) Request‑level description\n",
        "            if rdesc := req.get(\"description\"):\n",
        "                buckets.append(\"**Request Description:**\\n\" + rdesc.strip())\n",
        "\n",
        "            # 2b) Endpoint line\n",
        "            if method and raw_url:\n",
        "                buckets.append(f\"**Endpoint:** `{method} {raw_url}`\")\n",
        "\n",
        "            # 2c) Headers\n",
        "            if hdrs := req.get(\"header\"):\n",
        "                lines = [\n",
        "                    f\"- `{h.get('key')}`: `{h.get('value')}`\"\n",
        "                    for h in hdrs\n",
        "                ]\n",
        "                buckets.append(\"**Headers:**\\n\" + \"\\n\".join(lines))\n",
        "\n",
        "            # 2d) Request body\n",
        "            if raw_body := req.get(\"body\", {}).get(\"raw\"):\n",
        "                buckets.append(\n",
        "                    \"**Request Body:**\\n```json\\n\"\n",
        "                    + raw_body[:2000]\n",
        "                    + \"\\n```\"\n",
        "                )\n",
        "\n",
        "            # 3) Response samples\n",
        "            for resp in cur.get(\"response\", []):\n",
        "                status = resp.get(\"code\") or resp.get(\"status\", \"\")\n",
        "                if body := resp.get(\"body\"):\n",
        "                    label = f\"**Response ({status})**\" if status else \"**Response**\"\n",
        "                    buckets.append(\n",
        "                        f\"{label}:\\n```json\\n\"\n",
        "                        + body[:2000]\n",
        "                        + \"\\n```\"\n",
        "                    )\n",
        "\n",
        "            # 4) Event scripts (tests/examples)\n",
        "            for ev in cur.get(\"event\", []):\n",
        "                exec_lines = ev.get(\"script\", {}).get(\"exec\") or []\n",
        "                if exec_lines:\n",
        "                    buckets.append(\n",
        "                        \"**Example Script:**\\n```javascript\\n\"\n",
        "                        + \"\\n\".join(exec_lines)[:2000]\n",
        "                        + \"\\n```\"\n",
        "                    )\n",
        "\n",
        "            # If we have any extracted pieces, save them\n",
        "            if buckets:\n",
        "                out.append({\n",
        "                    \"content\": (\n",
        "                        f\"# {' › '.join(new_path)}\\n\\n\"\n",
        "                        + \"\\n\\n\".join(buckets)\n",
        "                    ),\n",
        "                    \"path\": \" › \".join(new_path)\n",
        "                })\n",
        "\n",
        "            # Recurse into child items\n",
        "            for child in cur.get(\"item\", []):\n",
        "                stack.append((new_path, child))\n",
        "\n",
        "        elif isinstance(cur, list):\n",
        "            for itm in cur:\n",
        "                stack.append((path, itm))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "# Run the collector and show a quick summary\n",
        "docs = collect_docs(root)\n",
        "print(\"Total docs captured:\", len(docs))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atx7CSq4LeXi",
        "outputId": "713b9eeb-d0ed-4981-ebc5-cd48d0a87345"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total docs captured: 392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block Summary**\n",
        "\n",
        "This cell configures and demonstrates how to split raw documentation entries into manageable chunks for embedding and retrieval:\n",
        "\n",
        "- **Importing text splitters**  \n",
        "  - `RecursiveCharacterTextSplitter` (general‑purpose, prioritizes paragraphs and lines)  \n",
        "  - `MarkdownTextSplitter` (optional, preserves markdown headings and fenced code)  \n",
        "\n",
        "- **Configuring the splitter**  \n",
        "  - `chunk_size=3000` characters target  \n",
        "  - `chunk_overlap=150` characters overlap to boost retrieval recall  \n",
        "  - `separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]` to split on paragraphs, then lines, then words  \n",
        "\n",
        "- **Defining `smart_chunk_docs(docs)`**  \n",
        "  - Iterates over each doc entry  \n",
        "  - Uses the splitter to break `content` into `pieces`  \n",
        "  - Attaches metadata `{ \"path\": ..., \"chunk\": index }` to each piece  \n",
        "\n",
        "- **Demonstration**  \n",
        "  - `docs_raw = collect_docs(root)` gathers ~392 full entries  \n",
        "  - `docs_chunk = smart_chunk_docs(docs_raw)` produces ~517 chunks  \n",
        "  - Prints before/after counts and an example metadata dict  \n",
        "\n",
        "- **Role in Pipeline**  \n",
        "  Splitting ensures each chunk is small enough (<3000 chars) for embedding models while maintaining context, improving FAISS similarity search for downstream prompt construction.  \n"
      ],
      "metadata": {
        "id": "qzp8Tn80wl1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownTextSplitter\n",
        "\n",
        "# NOTE: We’re not using smart_chunk_docs here because our collected docs\n",
        "# are already well‑structured and sized appropriately for embedding.\n",
        "# In general, for unstructured text, you could split into ~3k‑char chunks:\n",
        "# splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=150, separators=[\"\\n\\n\",\"\\n\",\" \",\"\"])\n",
        "# or use MarkdownTextSplitter to preserve headings/code fences.\n",
        "#\n",
        "# But for our Postman‑derived docs, each entry is its own “chunk” and passed\n",
        "# directly to the embedder.\n",
        "\n",
        "def smart_chunk_docs(docs):\n",
        "    \"\"\"\n",
        "    (Optional) Split large docs into overlapping sub‑chunks for retrieval.\n",
        "    Not used in this pipeline since each doc entry is already appropriately sized.\n",
        "    \"\"\"\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=3000,\n",
        "        chunk_overlap=150,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
        "    )\n",
        "    out = []\n",
        "    for d in docs:\n",
        "        pieces = splitter.split_text(d[\"content\"])\n",
        "        meta   = d.get(\"meta\") or {\"path\": d.get(\"path\", \"\")}\n",
        "        for i, part in enumerate(pieces):\n",
        "            out.append({\"content\": part, \"meta\": {**meta, \"chunk\": i}})\n",
        "    return out\n",
        "\n",
        "# Use the raw docs directly for embedding/retrieval:\n",
        "docs_raw = collect_docs(root)         # e.g. 392 entries\n",
        "print(\"Docs to index:\", len(docs_raw))\n",
        "docs_chunk = smart_chunk_docs(docs_raw)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LWGnlDYsLg3L",
        "outputId": "a622d274-4bdf-4cd2-ca6d-6754741d9d3e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Docs to index: 392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block Summary**\n",
        "\n",
        "This cell builds our FAISS vector index by embedding each documentation chunk:\n",
        "\n",
        "- **Client setup**  \n",
        "  - Uses the `OpenAI` client with your `api_key` for embedding calls.  \n",
        "\n",
        "- **Defining `embed_texts`**  \n",
        "  - Wrapper around the new OpenAI ≥1.0.0 SDK  \n",
        "  - Takes a list of strings plus an embedding model name  \n",
        "  - Returns a list of float vectors in the same order as inputs  \n",
        "\n",
        "- **Batch embedding loop**  \n",
        "  - Splits `docs_raw` into batches of size 96  \n",
        "  - Calls `embed_texts` for each batch to avoid rate or size limits  \n",
        "  - Accumulates all embeddings into `vecs`  \n",
        "\n",
        "- **FAISS index creation**  \n",
        "  - Converts `vecs` to a NumPy array of type `float32`  \n",
        "  - Initializes an inner‑product index (`IndexFlatIP`) matching the embedding dimension  \n",
        "  - Adds all vectors to the index for fast similarity search  \n",
        "\n",
        "- **Verification**  \n",
        "  - Prints the total number of vectors indexed (`index.ntotal`)  \n",
        "\n",
        "This prepares the FAISS index for nearest‑neighbor retrieval in downstream prompt construction.\n"
      ],
      "metadata": {
        "id": "jnnHZB_vxNDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "def embed_texts(texts: list[str], model: str = EMBED_MODEL) -> list[list[float]]:\n",
        "    \"\"\"\n",
        "    Returns embeddings for a list of texts using the OpenAI SDK client.\n",
        "    \"\"\"\n",
        "    resp = client.embeddings.create(model=model, input=texts)\n",
        "    return [e.embedding for e in resp.data]\n",
        "\n",
        "# Batch‑embed all raw document chunks and build a FAISS index\n",
        "vecs, BATCH = [], 96\n",
        "for i in range(0, len(docs_raw), BATCH):\n",
        "    batch = [d[\"content\"] for d in docs_raw[i:i+BATCH]]\n",
        "    vecs.extend(embed_texts(batch))\n",
        "\n",
        "# Convert to NumPy float32 array and index with inner‑product similarity\n",
        "vecs = np.asarray(vecs, dtype=\"float32\")\n",
        "index = faiss.IndexFlatIP(vecs.shape[1])\n",
        "index.add(vecs)\n",
        "\n",
        "print(\"FAISS index size:\", index.ntotal)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Fku7VWSLiDc",
        "outputId": "e4647de1-5230-4fd9-ed24-a0fcade53264"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FAISS index size: 392\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block Summary**  \n",
        "\n",
        "This helper function lets you query the raw Sikka API documentation “knowledge base” without triggering the full pipeline. It:\n",
        "\n",
        "- **Embeds the user query**  \n",
        "  - Calls `embed_texts` to get the query vector  \n",
        "- **Performs a FAISS lookup**  \n",
        "  - Retrieves the top `k` most similar document chunks  \n",
        "- **Builds a minimal LLM prompt**  \n",
        "  - System instruction: “Answer only from context; say ‘Not found’ otherwise.”  \n",
        "  - User message: includes the retrieved context and the original question  \n",
        "- **Invokes the chat model**  \n",
        "  - Uses `gpt-4o-mini` and returns the assistant’s text response  \n",
        "\n",
        "You can call `ask_sikka(\"your question\")` to test how well the vector store + LLM answers direct questions from the docs.\n"
      ],
      "metadata": {
        "id": "TZpM5eZFx_Nn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_sikka(\n",
        "    query: str,\n",
        "    k: int = 4,\n",
        "    model: str = \"gpt-4o-mini\",\n",
        "    debug: bool = False\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Retrieve the top‑k relevant chunks for `query` from your FAISS index,\n",
        "    then ask the chat model to answer using ONLY that context.\n",
        "\n",
        "    Args:\n",
        "      query: The user’s question.\n",
        "      k: Number of chunks to retrieve.\n",
        "      model: Which chat model to use.\n",
        "      debug: If True, print each retrieved chunk with its score.\n",
        "\n",
        "    Returns:\n",
        "      The assistant’s reply (stripped of whitespace).\n",
        "    \"\"\"\n",
        "    # 1) Embed the query\n",
        "    embedding: List[float] = embed_texts([query])[0]\n",
        "\n",
        "    # 2) FAISS search for top-k similar docs\n",
        "    distances, indices = index.search(\n",
        "        np.array([embedding], dtype=\"float32\"),\n",
        "        k\n",
        "    )\n",
        "\n",
        "    # 3) Debug output of retrieved snippets\n",
        "    if debug:\n",
        "        for dist, idx in zip(distances[0], indices[0]):\n",
        "            snippet = docs_chunk[idx][\"content\"]\n",
        "            print(f\"[DEBUG] Chunk {idx} (score {dist:.3f}):\\n{snippet}\\n\")\n",
        "\n",
        "    # 4) Build the context string\n",
        "    context = \"\\n\\n---\\n\\n\".join(\n",
        "        docs_chunk[i][\"content\"] for i in indices[0]\n",
        "    )\n",
        "\n",
        "    # 5) Ask the model with a strict “only-from-context” instruction\n",
        "    response = client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"Answer **only** from the context below; if it's not in the context, reply 'Not found'.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query}\"\n",
        "            }\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # 6) Return the assistant’s reply\n",
        "    return response.choices[0].message.content.strip()\n",
        "\n",
        "\n",
        "# ── Example Usage ─────────────────────────────────────────────────────\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    for q in [\n",
        "        \"For the POST /v2/payment/store-card endpoint, list its headers and parameters.\",\n",
        "        \"What is the endpoint for save a card?\"\n",
        "    ]:\n",
        "        print(f\"Q: {q}\")\n",
        "        print(\"A:\", ask_sikka(q))\n",
        "        # print(\"A:\", ask_sikka(q, debug=True))\n",
        "        print(\"-\" * 40)"
      ],
      "metadata": {
        "id": "90WpqVXALj7v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc9fcca0-d4cb-4c0f-dabc-a353735638c8"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: For the POST /v2/payment/store-card endpoint, list its headers and parameters.\n",
            "A: **Headers:**\n",
            "- `Content-Type`: `application/json`\n",
            "\n",
            "**Parameters:**\n",
            "- `request_key`: \"mandatory\"\n",
            "- `patient_id`: \"mandatory\"\n",
            "- `guarantor_id`: \"mandatory\"\n",
            "- `practice_id`: \"mandatory\"\n",
            "- `cust_id`: \"mandatory\"\n",
            "- `provider_id`: \"optional\"\n",
            "- `name`: \"mandatory\"\n",
            "- `zipcode`: \"mandatory\"\n",
            "- `billing_id`: \"mandatory\"\n",
            "- `is_update_default`: \"mandatory\"\n",
            "- `is_default`: \"true/false\"\n",
            "----------------------------------------\n",
            "Q: What is the endpoint for save a card?\n",
            "A: `POST https://api.sikkasoft.com/v2/payment/store_card`\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block Summary**\n",
        "\n",
        "This cell defines the `OPTIMIZER_TEMPLATE` string, which primes the Query Optimizer Agent to:\n",
        "\n",
        "- **Validate Scope**  \n",
        "  - Checks if the user’s request pertains to building or integrating with Sikka APIs.  \n",
        "  - If out‑of‑scope, instructs the agent to output a minimal JSON and stop.\n",
        "\n",
        "- **Detect Tech Stack**  \n",
        "  - Parses the user message for frontend frameworks/languages (e.g. React, Next.js, TypeScript).  \n",
        "  - Parses for backend frameworks/languages (e.g. Node.js, Express, Python, Flask).  \n",
        "  - Falls back to “React + JavaScript” front end and “Node.js + Express + JavaScript” back end if none are detected.\n",
        "\n",
        "- **Rewrite & Decompose**  \n",
        "  - Produces a one‑sentence `optimized_query` restating the user’s goal.  \n",
        "  - Generates **four** specialized prompts under `prompts` for downstream agents:\n",
        "    1. **api_docs** – Instructs how to identify and document all required Sikka endpoints, including full base URLs, auth flow, headers, parameters, and examples.  \n",
        "    2. **frontend** – Guides the UI agent to build the frontend with the detected framework, integrate the endpoints, and handle state/loading/errors.  \n",
        "    3. **backend** – Directs the backend agent to implement routes in the detected framework, handle `request_key` acquisition/refresh, and call the endpoints returning JSON.  \n",
        "    4. **formatter** – Tells the formatter agent to assemble everything into a single Markdown document with specified headings.\n",
        "\n",
        "- **Define JSON Output Schema**  \n",
        "  - Specifies the exact structure the optimizer must return, containing:  \n",
        "    ```json\n",
        "    {\n",
        "      \"scope_ok\": true|false,\n",
        "      \"optimized_query\": \"...\",\n",
        "      \"tech_stack\": {\"frontend\": \"...\", \"backend\": \"...\"},\n",
        "      \"prompts\": {\n",
        "        \"api_docs\": \"...\",\n",
        "        \"frontend\": \"...\",\n",
        "        \"backend\": \"...\",\n",
        "        \"formatter\": \"...\"\n",
        "      }\n",
        "    }\n",
        "    ```  \n",
        "  - Emphasizes “no extra keys” to ensure downstream agents receive a strictly consistent payload.\n"
      ],
      "metadata": {
        "id": "5wjUY7o48mhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt template  ─────────────────────────────────────────────\n",
        "\n",
        "OPTIMIZER_TEMPLATE = \"\"\"\n",
        "You are the Query Optimizer for a multi‑agent system that builds full‑stack apps with Sikka APIs.\n",
        "\n",
        "USER_MESSAGE:\n",
        "\\\"\\\"\\\"{user_message}\\\"\\\"\\\"\n",
        "\n",
        "Step 1: Scope\n",
        "• If the message is not about building or integrating Sikka APIs, output exactly:\n",
        "  {{ \"scope_ok\": false, \"reason\": \"short explanation\" }}\n",
        "  and STOP.\n",
        "\n",
        "Step 2: Tech stack\n",
        "• Detect any frontend frameworks/languages (e.g. React, Next.js, TypeScript).\n",
        "• Detect any backend frameworks/languages (e.g. Node.js, Express, Python, Flask).\n",
        "• If none are found, default to “React + JavaScript” front end and “Node.js + Express + JavaScript” back end.\n",
        "\n",
        "Step 3: Rewrite & decompose\n",
        "• Produce one sentence `optimized_query` restating the goal.\n",
        "• Produce **four** prompts under `prompts` with these keys:\n",
        "\n",
        "  1. **api_docs**\n",
        "     – Identify all required Sikka endpoints for `optimized_query`.\n",
        "     – For each endpoint, specify the **full base URL including version** (e.g. `https://api.sikkasoft.com/v4`), the path, and HTTP method.\n",
        "     – Document the authentication flow (request_key lifecycle).\n",
        "     – List required headers and body/query parameters per endpoint.\n",
        "     – Include a sample request and a sample response for each endpoint.\n",
        "\n",
        "  2. **frontend**\n",
        "     – Build the UI using the detected frontend framework.\n",
        "     – Call the endpoints identified in `api_docs`.\n",
        "     – Specify component/file names.\n",
        "     – Handle form state, loading indicators, and error displays.\n",
        "\n",
        "  3. **backend**\n",
        "     – Implement server routes using the detected backend framework.\n",
        "     – Include code to obtain and refresh the request_key.\n",
        "     – Show how to call each Sikka endpoint and return JSON responses.\n",
        "\n",
        "  4. **formatter**\n",
        "     – Assemble **all** outputs into a single Markdown document.\n",
        "     – Include headings for Overview, API Documentation, Frontend Code, and Backend Code.\n",
        "\n",
        "Step 4: Output exactly this JSON (no extra keys):\n",
        "\n",
        "```json\n",
        "{{\n",
        "  \"scope_ok\": true,\n",
        "  \"optimized_query\": \"<one‑sentence restatement>\",\n",
        "  \"tech_stack\": {{\n",
        "    \"frontend\": \"<detected or default stack>\",\n",
        "    \"backend\":  \"<detected or default stack>\"\n",
        "  }},\n",
        "  \"prompts\": {{\n",
        "    \"api_docs\":  \"<instruction containing all required elements>\",\n",
        "    \"frontend\":  \"<instruction containing all required elements>\",\n",
        "    \"backend\":   \"<instruction containing all required elements>\",\n",
        "    \"formatter\": \"<instruction containing all required elements>\"\n",
        "  }}\n",
        "}}\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "b9Xn1yBwLkyi"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block Summary**\n",
        "\n",
        "This cell sets up and runs the **Query Optimizer** agent:\n",
        "\n",
        "- **Imports & Client Setup**  \n",
        "  - `Agent`, `Task`, `Crew` from `crewai` to define and execute tasks.  \n",
        "  - `re` for stripping Markdown fences from the LLM output.  \n",
        "  - `OpenAI` client (v1 SDK) initialized with the Colab secret.  \n",
        "\n",
        "- **Agent Definition**  \n",
        "  - Creates `optimizer_agent` with role, goal, backstory, and LLM spec (`gpt-4o-mini`).  \n",
        "  - Defines a reusable `opt_task` template pointing at `OPTIMIZER_TEMPLATE`.  \n",
        "\n",
        "- **Helper Function `run_optimizer`**  \n",
        "  1. **Prompt Formatting**  \n",
        "     - Injects the `user_message` into the `OPTIMIZER_TEMPLATE`.  \n",
        "  2. **Task Execution**  \n",
        "     - Constructs and runs a one‑task `Crew` to invoke the optimizer.  \n",
        "  3. **Output Cleaning**  \n",
        "     - Strips any ```json fences.  \n",
        "  4. **JSON Parsing**  \n",
        "     - Returns a Python `dict` matching the optimizer’s schema.  \n",
        "\n",
        "- **Example Invocation**  \n",
        "  - Demonstrates using `run_optimizer(...)` to validate/normalize a sample user request.\n"
      ],
      "metadata": {
        "id": "98-QnuP185Hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "from crewai import Agent, Task, Crew\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Load API key from Colab secrets and initialize OpenAI client\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "# 1) Define the Query Optimizer agent\n",
        "optimizer_agent = Agent(\n",
        "    role=\"Query Optimizer\",\n",
        "    goal=\"Validate scope and normalize user queries about Sikka APIs\",\n",
        "    backstory=\"Expert in Sikka’s API portfolio and software design.\",\n",
        "    allow_delegation=False,\n",
        "    llm=\"gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "# 2) Prepare the optimization task template\n",
        "opt_task = Task(\n",
        "    agent=optimizer_agent,\n",
        "    description=OPTIMIZER_TEMPLATE,    # will .format(user_message=…) later\n",
        "    expected_output=\"JSON exactly matching the schema above.\"\n",
        ")\n",
        "\n",
        "def run_optimizer(user_message: str) -> dict:\n",
        "    \"\"\"\n",
        "    Execute the optimizer on `user_message` and parse its JSON output.\n",
        "    \"\"\"\n",
        "    # a) Inject the user's message into the template\n",
        "    prompt = OPTIMIZER_TEMPLATE.format(user_message=user_message)\n",
        "\n",
        "    # b) Create & run the task\n",
        "    task = Task(\n",
        "        agent=optimizer_agent,\n",
        "        description=prompt,\n",
        "        expected_output=\"JSON exactly matching the schema above.\"\n",
        "    )\n",
        "    crew_output = Crew(agents=[optimizer_agent], tasks=[task], verbose=False).kickoff()\n",
        "\n",
        "    # c) Extract raw LLM response\n",
        "    raw = crew_output.tasks_output[0].raw\n",
        "\n",
        "    # d) Strip any ``` or ```json fences\n",
        "    cleaned = \"\\n\".join(\n",
        "        line for line in raw.splitlines()\n",
        "        if not re.match(r\"^```(?:json)?\\s*$\", line)\n",
        "    ).strip()\n",
        "\n",
        "    # e) Parse and return as a dict\n",
        "    return json.loads(cleaned)\n",
        "\n",
        "# Example: Using the Query Optimizer on a user request\n",
        "if __name__ == \"__main__\":\n",
        "    # 1) Define the user's natural‑language request\n",
        "    user_request = (\n",
        "        \"I want a React checkout page that saves a patient's card \"\n",
        "        \"and runs a $25 sale using the Sikka sandbox.\"\n",
        "    )\n",
        "\n",
        "    # 2) Print the original request string\n",
        "    print(\"=== User Request ===\")\n",
        "    print(user_request, \"\\n\")\n",
        "\n",
        "    # 3) Run the optimizer to normalize & decompose the request\n",
        "    optimized_output = run_optimizer(user_request)\n",
        "\n",
        "    # 4) Pretty‑print the optimized result for clarity\n",
        "    import json\n",
        "    print(\"=== Optimizer Result ===\")\n",
        "    print(json.dumps(optimized_output, indent=2))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2ILqBcFLlzS",
        "outputId": "ae569f02-5b03-4622-c65d-5d354893422a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== User Request ===\n",
            "I want a React checkout page that saves a patient's card and runs a $25 sale using the Sikka sandbox. \n",
            "\n",
            "=== Optimizer Result ===\n",
            "{\n",
            "  \"scope_ok\": true,\n",
            "  \"optimized_query\": \"Create a React checkout page that saves a patient's card and processes a $25 sale using the Sikka sandbox.\",\n",
            "  \"tech_stack\": {\n",
            "    \"frontend\": \"React + JavaScript\",\n",
            "    \"backend\": \"Node.js + Express + JavaScript\"\n",
            "  },\n",
            "  \"prompts\": {\n",
            "    \"api_docs\": \"Identify and document required Sikka endpoints for processing card savings and completing a $25 sale. Base URL: https://api.sikkasoft.com/v4. Include the following endpoints: 1) POST /payment/process for processing a sale (http request), with necessary headers (Authorization: Bearer {request_key}) and body parameters (amount, card_info). Sample request: {\\\"amount\\\": 2500, \\\"card_info\\\": {\\\"card_number\\\": \\\"4111111111111111\\\", \\\"expiry\\\": \\\"12/24\\\", \\\"cvv\\\": \\\"123\\\"}}. Sample response: {\\\"status\\\": \\\"success\\\", \\\"transaction_id\\\": \\\"abcd1234\\\"}. Include authentication flow explaining request_key lifecycle.\",\n",
            "    \"frontend\": \"Use React to build a checkout UI that includes card input fields, a button to submit the payment, and error handling vis-a-vis loading indicators and messages. Component names: CheckoutForm.jsx for the main form, CardInput.jsx for card details. Use hooks to manage form state.\",\n",
            "    \"backend\": \"Implement server routes using Node.js and Express to handle requests from the frontend. Create POST route '/api/payment' to receive card information from the frontend, call the Sikka payment processing API, manage request_key acquisition and refreshing, and return a JSON response back to the frontend.\",\n",
            "    \"formatter\": \"Assemble all outputs into a single Markdown document with the following headings: Overview, API Documentation (including endpoint details and sample requests/responses), Frontend Code (with UI component code examples), and Backend Code (with Express server example).\"\n",
            "  }\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block Summary**\n",
        "\n",
        "This cell sets up and runs the “API‑Docs Generator” agent:\n",
        "\n",
        "- **Imports**  \n",
        "  - Pulls in `Agent`, `Task`, and `Crew` from `crewai` to define and execute the agent.  \n",
        "  - Imports `numpy` for array handling in the FAISS search.\n",
        "\n",
        "- **Context Retriever**  \n",
        "  - `retrieve_api_context(query, k)` embeds the user’s query, searches the FAISS index for the top‑k most relevant chunks, and concatenates them as a context string.\n",
        "\n",
        "- **Agent Definition**  \n",
        "  - `api_doc_agent` is configured to generate detailed Markdown API documentation for Sikka endpoints.\n",
        "\n",
        "- **Runner Function**  \n",
        "  - `run_api_docs_step(opt_out)`  \n",
        "    1. Checks if the user request is in‑scope.  \n",
        "    2. Builds a prompt combining the optimizer’s `api_docs` instruction with FAISS‑retrieved context.  \n",
        "    3. Runs a single‑task Crew to get the Markdown documentation.  \n",
        "    4. Returns the raw Markdown response for downstream use.\n"
      ],
      "metadata": {
        "id": "zV7RXdeY96bT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Agent, Task, Crew   # Core classes to define and run our agent\n",
        "import numpy as np                    # Numerical library used by FAISS search\n",
        "\n",
        "def retrieve_api_context(query: str, k: int = 6) -> str:\n",
        "    \"\"\"\n",
        "    Embed the query, perform a FAISS similarity search,\n",
        "    and join the top-k document chunks into one context string.\n",
        "    \"\"\"\n",
        "    q_emb = embed_texts([query])[0]                      # 1) Create embedding for the query\n",
        "    _, idx = index.search(np.array([q_emb], dtype=\"float32\"), k)  # 2) Search FAISS index\n",
        "    # 3) Concatenate the retrieved passages with separators\n",
        "    return \"\\n\\n---\\n\\n\".join(docs[i][\"content\"] for i in idx[0])\n",
        "\n",
        "# Define the API‑Docs Generator agent\n",
        "api_doc_agent = Agent(\n",
        "    role             = \"API Docs Generator\",    # Agent’s persona\n",
        "    goal             = \"Produce detailed API documentation for the endpoints needed\",\n",
        "    backstory        = \"Specialist in Sikka API reference docs.\",\n",
        "    allow_delegation = False,\n",
        "    llm              = \"gpt-4o-mini\"\n",
        ")\n",
        "\n",
        "def run_api_docs_step(opt_out: dict) -> str:\n",
        "    \"\"\"\n",
        "    Build and run a single‑task Crew to generate Markdown API docs.\n",
        "    Returns the raw Markdown or an out‑of‑scope notice.\n",
        "    \"\"\"\n",
        "    # 1) Scope guard: skip if optimizer said out‑of‑scope\n",
        "    if not opt_out.get(\"scope_ok\", False):\n",
        "        return \"⚠️ Out of scope. No API docs generated.\"\n",
        "\n",
        "    # 2) Prepare prompt: instruction + retrieved context\n",
        "    instr  = opt_out[\"prompts\"][\"api_docs\"]                   # Optimizer’s API‑docs instruction\n",
        "    ctx    = retrieve_api_context(opt_out[\"optimized_query\"]) # FAISS context for the optimized query\n",
        "    prompt = f\"{instr}\\n\\nHere is the relevant API context:\\n{ctx}\"  # Complete LLM prompt\n",
        "\n",
        "    # 3) Define and run the Crew task\n",
        "    task = Task(\n",
        "        agent           = api_doc_agent,\n",
        "        description     = prompt,\n",
        "        expected_output = (\n",
        "            \"A Markdown document covering ONLY the Sikka endpoints, \"\n",
        "            \"authentication flow, headers, parameters, and sample calls.\"\n",
        "        )\n",
        "    )\n",
        "    crew_output = Crew(\n",
        "        agents=[api_doc_agent],\n",
        "        tasks=[task],\n",
        "        verbose=False\n",
        "    ).kickoff()  # Execute the task\n",
        "\n",
        "    # 4) Extract and return the raw Markdown response\n",
        "    return crew_output.tasks_output[0].raw\n"
      ],
      "metadata": {
        "id": "g-Csxg0uLsA1"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block Summary**\n",
        "\n",
        "This cell defines and runs the “Frontend Code Generator” agent:\n",
        "\n",
        "- **Agent Definition**  \n",
        "  - Creates `frontend_agent` with a React/Next.js focus and clear UI/UX expertise.\n",
        "\n",
        "- **Runner Function (`run_frontend_step`)**  \n",
        "  1. **Scope Guard:** Returns early if the optimizer flagged the request as out‑of‑scope.  \n",
        "  2. **Prompt Assembly:** Combines the optimizer’s “frontend” instruction with the previously generated API docs.  \n",
        "  3. **Crew Execution:** Spins up a one‑task Crew to generate the React/Next.js code.  \n",
        "  4. **Output Extraction:** Returns the raw code string for use by subsequent steps.\n"
      ],
      "metadata": {
        "id": "Qp4bW7cg-GR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Agent, Task, Crew\n",
        "\n",
        "# 1) Define the Frontend Code Generator agent\n",
        "frontend_agent = Agent(\n",
        "    role             = \"Frontend Code Generator\",              # Agent’s persona\n",
        "    goal             = \"Produce React/Next.js frontend code for the checkout flow\",\n",
        "    backstory        = \"Expert in building modern React applications and UI/UX best practices.\",\n",
        "    allow_delegation = False,\n",
        "    llm              = \"gpt-4o-mini\"                         # Model to drive this agent\n",
        ")\n",
        "\n",
        "def run_frontend_step(opt_out: dict, api_docs_md: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate (or regenerate) the React/Next.js frontend code.\n",
        "\n",
        "    Args:\n",
        "      opt_out     – Output dict from run_optimizer(...)\n",
        "      api_docs_md – Markdown API docs from run_api_docs_step(...)\n",
        "\n",
        "    Returns:\n",
        "      Raw code string for the frontend application.\n",
        "    \"\"\"\n",
        "    # 1) Out‑of‑scope guard\n",
        "    if not opt_out.get(\"scope_ok\", False):\n",
        "        return \"⚠️ Out of scope. No frontend code generated.\"\n",
        "\n",
        "    # 2) Build the prompt using the optimizer’s 'frontend' instruction\n",
        "    instr  = opt_out[\"prompts\"][\"frontend\"]                   # Instruction text\n",
        "    prompt = (\n",
        "        f\"{instr}\\n\\nHere are the API docs you should integrate with:\\n\"\n",
        "        f\"{api_docs_md}\"\n",
        "    )\n",
        "\n",
        "    # 3) Launch a one‑task Crew to generate the code\n",
        "    task = Task(\n",
        "        agent           = frontend_agent,                     # Which agent to run\n",
        "        description     = prompt,                             # Full LLM prompt\n",
        "        expected_output = (\n",
        "            \"A React (or Next.js) component/file structure & code for the checkout UI, \"\n",
        "            \"handling card input, form state, loading, and error display.\"\n",
        "        )\n",
        "    )\n",
        "    crew_output = Crew(\n",
        "        agents=[frontend_agent],\n",
        "        tasks =[task],\n",
        "        verbose=False\n",
        "    ).kickoff()\n",
        "\n",
        "    # 4) Extract and return the raw code string\n",
        "    return crew_output.tasks_output[0].raw\n"
      ],
      "metadata": {
        "id": "7nO57VoDLtoe"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block Summary**\n",
        "\n",
        "This cell defines and runs the “Backend Code Generator” agent:\n",
        "\n",
        "- **Agent Definition**  \n",
        "  - Creates `backend_agent` specialized in Node.js/Express APIs with a focus on security and scalability.\n",
        "\n",
        "- **Runner Function (`run_backend_step`)**  \n",
        "  1. **Scope Guard:** Checks the optimizer output; returns early if out‑of‑scope.  \n",
        "  2. **Prompt Assembly:** Combines the optimizer’s “backend” instruction with the generated API docs.  \n",
        "  3. **Crew Execution:** Launches a one‑task Crew to generate the Express server code.  \n",
        "  4. **Output Extraction:** Returns the raw backend code string for downstream use.  \n"
      ],
      "metadata": {
        "id": "WEQSxeY--Rpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from crewai import Agent, Task, Crew\n",
        "\n",
        "# 1) Define the Backend Code Generator agent\n",
        "backend_agent = Agent(\n",
        "    role             = \"Backend Code Generator\",                 # Agent’s persona\n",
        "    goal             = \"Produce Node.js/Express backend code for the checkout flow\",\n",
        "    backstory        = \"Expert in building secure and scalable Express APIs.\",\n",
        "    allow_delegation = False,\n",
        "    llm              = \"gpt-4o-mini\"                          # Model to drive this agent\n",
        ")\n",
        "\n",
        "def run_backend_step(opt_out: dict, api_docs_md: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate the Node.js + Express backend code.\n",
        "\n",
        "    Args:\n",
        "      opt_out     – dict from run_optimizer(...)\n",
        "      api_docs_md – Markdown API docs from run_api_docs_step(...)\n",
        "\n",
        "    Returns:\n",
        "      Raw code string for the backend server.\n",
        "    \"\"\"\n",
        "    # 1) Out‑of‑scope guard\n",
        "    if not opt_out.get(\"scope_ok\", False):\n",
        "        return \"⚠️ Out of scope. No backend code generated.\"\n",
        "\n",
        "    # 2) Build the prompt using the optimizer’s 'backend' instruction\n",
        "    instr  = opt_out[\"prompts\"][\"backend\"]                      # Instruction text\n",
        "    prompt = (\n",
        "        f\"{instr}\\n\\nHere are the API docs you should integrate with:\\n\"\n",
        "        f\"{api_docs_md}\"\n",
        "    )\n",
        "\n",
        "    # 3) Launch a one‑task Crew to generate the code\n",
        "    task = Task(\n",
        "        agent           = backend_agent,                         # Which agent to run\n",
        "        description     = prompt,                                # Full LLM prompt\n",
        "        expected_output = (\n",
        "            \"Node.js + Express server code with routes to handle saving cards \"\n",
        "            \"and processing payments using the provided Sikka endpoints.\"\n",
        "        )\n",
        "    )\n",
        "    crew_output = Crew(\n",
        "        agents=[backend_agent],\n",
        "        tasks =[task],\n",
        "        verbose=False\n",
        "    ).kickoff()\n",
        "\n",
        "    # 4) Extract and return the raw code string\n",
        "    return crew_output.tasks_output[0].raw\n"
      ],
      "metadata": {
        "id": "U1kkqOVDLu68"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block Summary**\n",
        "\n",
        "This cell defines the **Frontend Code Evaluator** agent and its runner function:\n",
        "\n",
        "- **Agent Definition**  \n",
        "  - `frontend_evaluator_agent` is a specialist in React/Next.js best practices and front‑end security.  \n",
        "  - Its sole responsibility is to detect **critical** correctness or security issues.\n",
        "\n",
        "- **Runner Helper (`run_frontend_eval`)**  \n",
        "  1. **Scope Check:** Returns empty report if the request is out of scope.  \n",
        "  2. **Prompt Construction:** Crafts a strict JSON‑only prompt asking for issues and suggestions.  \n",
        "  3. **Crew Execution:** Runs a single‑task Crew to evaluate the provided code.  \n",
        "  4. **Fence Stripping:** Removes any Markdown code fences from the LLM response.  \n",
        "  5. **JSON Parsing:** Parses and returns the structured report for downstream use.\n"
      ],
      "metadata": {
        "id": "yfbH6NWm-rhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from crewai import Agent, Task, Crew\n",
        "\n",
        "# 1) Define the Frontend Evaluator agent\n",
        "frontend_evaluator_agent = Agent(\n",
        "    role             = \"Frontend Code Evaluator\",                  # Agent’s persona\n",
        "    goal             = (\n",
        "        \"Inspect the React/Next.js frontend code and report *only* critical \"\n",
        "        \"correctness or security issues in JSON format.\"\n",
        "    ),\n",
        "    backstory        = \"Expert in React best practices, form validation, and front‑end security.\",\n",
        "    allow_delegation = False,\n",
        "    llm              = \"gpt-4o-mini\"                               # Model to drive evaluation\n",
        ")\n",
        "\n",
        "# 2) Runner helper for frontend evaluation\n",
        "def run_frontend_eval(opt_out: dict, frontend_code: str) -> dict:\n",
        "    \"\"\"\n",
        "    Analyze frontend code for blocking issues and return a JSON report:\n",
        "    {\n",
        "      \"issues\": [ {\"location\":\"frontend\",\"line\":int,\"message\":str,\"severity\":\"critical\"}, ... ],\n",
        "      \"suggestions\": [ str, ... ]\n",
        "    }\n",
        "    \"\"\"\n",
        "    # Guard: skip evaluation if out-of-scope\n",
        "    if not opt_out.get(\"scope_ok\", False):\n",
        "        return {\"issues\": [], \"suggestions\": []}\n",
        "\n",
        "    # Build a strict JSON‑schema prompt\n",
        "    prompt = f\"\"\"\n",
        "You are a strict Bug‑Finder focused ONLY on frontend code.\n",
        "Report *only* critical correctness or security issues in this JSON schema:\n",
        "\n",
        "{{\n",
        "  \"issues\": [\n",
        "    {{\n",
        "      \"location\": \"frontend\",\n",
        "      \"line\": <integer>,\n",
        "      \"message\": \"<description>\",\n",
        "      \"severity\": \"critical\"\n",
        "    }},\n",
        "    ...\n",
        "  ],\n",
        "  \"suggestions\": [\n",
        "    \"<actionable improvement>\",\n",
        "    ...\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Here is the frontend code to evaluate:\n",
        "{frontend_code}\n",
        "\n",
        "Do NOT include any markdown or additional keys—output only the JSON.\n",
        "\"\"\".strip()\n",
        "\n",
        "    # Execute the evaluation task\n",
        "    task = Task(\n",
        "        agent           = frontend_evaluator_agent,\n",
        "        description     = prompt,\n",
        "        expected_output = \"A JSON object with keys 'issues' and 'suggestions'.\"\n",
        "    )\n",
        "    out = Crew(agents=[frontend_evaluator_agent], tasks=[task], verbose=False).kickoff()\n",
        "    raw = out.tasks_output[0].raw                                 # Raw LLM reply\n",
        "\n",
        "    # Remove any ``` code fences\n",
        "    cleaned = \"\\n\".join(\n",
        "        line for line in raw.splitlines()\n",
        "        if not re.match(r\"^```(?:\\\\w+)?\\\\s*$\", line)\n",
        "    ).strip()\n",
        "\n",
        "    # Parse and return the JSON issue report\n",
        "    return json.loads(cleaned)\n"
      ],
      "metadata": {
        "id": "jLoK-cpvLwHF"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block Summary**\n",
        "\n",
        "This cell defines the **Backend Code Evaluator** agent and its runner function:\n",
        "\n",
        "- **Agent Definition**  \n",
        "  - `backend_evaluator_agent` is an expert in Express API design, authentication flows, and security.  \n",
        "  - Its sole responsibility is to identify **critical** correctness or security issues in backend code.\n",
        "\n",
        "- **Runner Helper (`run_backend_eval`)**  \n",
        "  1. **Scope Check:** Returns an empty report if the request is out of scope.  \n",
        "  2. **Prompt Construction:** Builds a strict JSON‑only prompt asking for backend issues and suggestions.  \n",
        "  3. **Crew Execution:** Runs a single‑task Crew to evaluate the provided backend code.  \n",
        "  4. **Fence Stripping:** Removes any Markdown code fences from the LLM response.  \n",
        "  5. **JSON Parsing:** Parses and returns the structured report for use in the refinement pipeline.\n"
      ],
      "metadata": {
        "id": "3weYgEGG-xPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from crewai import Agent, Task, Crew\n",
        "\n",
        "# 1) Define the Backend Evaluator agent\n",
        "backend_evaluator_agent = Agent(\n",
        "    role             = \"Backend Code Evaluator\",                     # Agent persona\n",
        "    goal             = (\n",
        "        \"Inspect the Node.js/Express backend code and report *only* critical \"\n",
        "        \"correctness or security issues in JSON format.\"\n",
        "    ),\n",
        "    backstory        = \"Expert in Express API design, authentication flows, and backend security.\",\n",
        "    allow_delegation = False,\n",
        "    llm              = \"gpt-4o-mini\"                                 # Model for evaluation\n",
        ")\n",
        "\n",
        "# 2) Runner helper for backend evaluation\n",
        "def run_backend_eval(opt_out: dict, backend_code: str) -> dict:\n",
        "    \"\"\"\n",
        "    Analyze backend code for blocking issues.\n",
        "    Returns a dict with:\n",
        "      {\n",
        "        \"issues\": [\n",
        "          {\"location\":\"backend\",\"line\":int,\"message\":str,\"severity\":\"critical\"},\n",
        "          ...\n",
        "        ],\n",
        "        \"suggestions\": [\n",
        "          str, ...\n",
        "        ]\n",
        "      }\n",
        "    \"\"\"\n",
        "    # Guard: skip if out-of-scope\n",
        "    if not opt_out.get(\"scope_ok\", False):\n",
        "        return {\"issues\": [], \"suggestions\": []}\n",
        "\n",
        "    # Build a strict JSON‑schema prompt\n",
        "    prompt = f\"\"\"\n",
        "You are a strict Bug‑Finder focused ONLY on backend code.\n",
        "Report *only* critical correctness or security issues in this JSON schema:\n",
        "\n",
        "{{\n",
        "  \"issues\": [\n",
        "    {{\n",
        "      \"location\": \"backend\",\n",
        "      \"line\": <integer>,\n",
        "      \"message\": \"<description>\",\n",
        "      \"severity\": \"critical\"\n",
        "    }},\n",
        "    ...\n",
        "  ],\n",
        "  \"suggestions\": [\n",
        "    \"<actionable improvement>\",\n",
        "    ...\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Here is the backend code to evaluate:\n",
        "{backend_code}\n",
        "\n",
        "Do NOT include any markdown or additional keys—output only the JSON.\n",
        "\"\"\".strip()\n",
        "\n",
        "    # Execute the evaluation task\n",
        "    task = Task(\n",
        "        agent           = backend_evaluator_agent,\n",
        "        description     = prompt,\n",
        "        expected_output = \"A JSON object with keys 'issues' and 'suggestions'.\"\n",
        "    )\n",
        "    out = Crew(agents=[backend_evaluator_agent], tasks=[task], verbose=False).kickoff()\n",
        "    raw = out.tasks_output[0].raw                                  # Raw LLM reply\n",
        "\n",
        "    # Remove any ``` code fences\n",
        "    cleaned = \"\\n\".join(\n",
        "        line for line in raw.splitlines()\n",
        "        if not re.match(r\"^```(?:\\\\w+)?\\\\s*$\", line)\n",
        "    ).strip()\n",
        "\n",
        "    # Parse and return the JSON report\n",
        "    return json.loads(cleaned)\n"
      ],
      "metadata": {
        "id": "0nawYk5eLxFD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block Summary**\n",
        "\n",
        "This cell defines a **reusable Code Refinement Agent** and its helper function:\n",
        "\n",
        "- **Agent Definition**  \n",
        "  - `refine_agent` is an expert in iterative code improvement and best practices.  \n",
        "  - Its goal is to take evaluator feedback and produce a revised code snippet (frontend or backend) without any extra text.\n",
        "\n",
        "- **Runner Helper (`run_refine_step`)**  \n",
        "  1. **Scope Check:** Returns the original snippet if out of scope.  \n",
        "  2. **Feedback Preparation:** Converts suggestion strings into a bullet‑list (not used directly in prompt but illustrative).  \n",
        "  3. **Prompt Construction:** Includes the evaluator report and original code, asks for only the refined code.  \n",
        "  4. **Crew Execution:** Runs a single‑task Crew to generate the updated code.  \n",
        "  5. **Fence Stripping:** Removes any ``` code fences from the response.  \n",
        "  6. **Return:** Outputs the cleaned, refined code snippet.\n"
      ],
      "metadata": {
        "id": "_ERy8Wnj-3_s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import re\n",
        "from crewai import Agent, Task, Crew\n",
        "\n",
        "# 1) Define the Code Refinement Agent\n",
        "refine_agent = Agent(\n",
        "    role             = \"Code Refinement Agent\",        # Agent persona\n",
        "    goal             = (\n",
        "        \"Incorporate evaluator feedback into an existing code snippet \"\n",
        "        \"and return the revised code only.\"\n",
        "    ),\n",
        "    backstory        = \"Expert in iterative code improvement and best practices.\",\n",
        "    allow_delegation = False,                         # Keep tasks internal\n",
        "    llm              = \"gpt-4o-mini\"                  # Model to run refinements\n",
        ")\n",
        "\n",
        "def run_refine_step(\n",
        "    opt_out: dict,           # Output from the optimizer step\n",
        "    code_snippet: str,       # The code to refine (frontend or backend)\n",
        "    eval_report: dict,       # JSON with 'issues' and 'suggestions'\n",
        "    code_type: str           # \"frontend\" or \"backend\"\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Refine a code snippet by applying the evaluator's suggestions.\n",
        "    Returns only the updated code (no explanations).\n",
        "    \"\"\"\n",
        "    # 2) Out‑of‑scope guard\n",
        "    if not opt_out.get(\"scope_ok\", False):\n",
        "        return code_snippet\n",
        "\n",
        "    # 3) Build the prompt with evaluator report and original code\n",
        "    prompt = f\"\"\"\n",
        "You are a Code Refinement Agent. Update the following {code_type} code using the evaluator feedback.\n",
        "\n",
        "Evaluator Report:\n",
        "{json.dumps(eval_report, indent=2)}\n",
        "\n",
        "Original {code_type.capitalize()} Code:\n",
        "```\n",
        "{code_snippet}\n",
        "```\n",
        "\n",
        "Please return **only** the refined {code_type} code, applying all critical fixes. Do not include any explanations.\n",
        "\"\"\".strip()\n",
        "\n",
        "    # 4) Create and run the refinement task\n",
        "    task = Task(\n",
        "        agent           = refine_agent,\n",
        "        description     = prompt,\n",
        "        expected_output = f\"The updated {code_type} code snippet, with fixes applied.\"\n",
        "    )\n",
        "    crew_output = Crew(agents=[refine_agent], tasks=[task], verbose=False).kickoff()\n",
        "    raw = crew_output.tasks_output[0].raw  # LLM response\n",
        "\n",
        "    # 5) Strip any ``` code fences from the response\n",
        "    cleaned = \"\\n\".join(\n",
        "        line for line in raw.splitlines()\n",
        "        if not re.match(r\"^```(?:\\w+)?\\s*$\", line)\n",
        "    ).strip()\n",
        "\n",
        "    # 6) Return the cleaned, refined code\n",
        "    return cleaned\n"
      ],
      "metadata": {
        "id": "1RVUKkAZLyBk"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block Summary**\n",
        "\n",
        "This cell defines the **parallelized end‑to‑end pipelines** for both frontend and backend:\n",
        "\n",
        "- **`_pipeline_frontend`**  \n",
        "  1. **Generate** initial React code  \n",
        "  2. **Evaluate** it for critical issues (prints debug report)  \n",
        "  3. **Refine** the code if any blocking issues are found  \n",
        "  4. **Return** the final, possibly refined frontend snippet  \n",
        "\n",
        "- **`_pipeline_backend`**  \n",
        "  1. **Generate** initial Express code  \n",
        "  2. **Evaluate** it for critical issues (prints debug report)  \n",
        "  3. **Refine** the code if needed  \n",
        "  4. **Return** the final backend snippet  \n",
        "\n",
        "- **`run_frontend_and_backend`**  \n",
        "  - Uses `ThreadPoolExecutor` to run both pipelines **concurrently**  \n",
        "  - Returns a tuple: `(final_frontend_code, final_backend_code)`  \n",
        "\n",
        "By parallelizing, this cell speeds up the combined generation‑evaluate‑refine workflow for both layers.\n"
      ],
      "metadata": {
        "id": "0UOaxYru_UuW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "def _pipeline_frontend(opt_out: dict, api_docs_md: str) -> str:\n",
        "    \"\"\"\n",
        "    Full frontend pipeline: generate → evaluate → refine.\n",
        "    Returns the final refined frontend code.\n",
        "    \"\"\"\n",
        "    # 1) Generate initial React/Next.js code\n",
        "    code = run_frontend_step(opt_out, api_docs_md)\n",
        "\n",
        "    # 2) Evaluate for critical issues\n",
        "    eval_report = run_frontend_eval(opt_out, code)\n",
        "    print(\"[DEBUG] Frontend eval_report:\", eval_report, flush=True)\n",
        "\n",
        "    # 3) Refine if any critical issues were reported\n",
        "    if eval_report[\"issues\"]:\n",
        "        code = run_refine_step(opt_out, code, eval_report, code_type=\"frontend\")\n",
        "\n",
        "    return code\n",
        "\n",
        "def _pipeline_backend(opt_out: dict, api_docs_md: str) -> str:\n",
        "    \"\"\"\n",
        "    Full backend pipeline: generate → evaluate → refine.\n",
        "    Returns the final refined backend code.\n",
        "    \"\"\"\n",
        "    # 1) Generate initial Node.js/Express code\n",
        "    code = run_backend_step(opt_out, api_docs_md)\n",
        "\n",
        "    # 2) Evaluate for critical issues\n",
        "    eval_report = run_backend_eval(opt_out, code)\n",
        "    print(\"[DEBUG] Backend eval_report:\", eval_report, flush=True)\n",
        "\n",
        "    # 3) Refine if any critical issues were reported\n",
        "    if eval_report[\"issues\"]:\n",
        "        code = run_refine_step(opt_out, code, eval_report, code_type=\"backend\")\n",
        "\n",
        "    return code\n",
        "\n",
        "def run_frontend_and_backend(opt_out: dict, api_docs_md: str) -> tuple[str, str]:\n",
        "    \"\"\"\n",
        "    Execute both frontend and backend pipelines in parallel.\n",
        "    Returns a tuple: (final_frontend_code, final_backend_code).\n",
        "    \"\"\"\n",
        "    with ThreadPoolExecutor(max_workers=2) as executor:\n",
        "        fut_frontend = executor.submit(_pipeline_frontend, opt_out, api_docs_md)\n",
        "        fut_backend  = executor.submit(_pipeline_backend,  opt_out, api_docs_md)\n",
        "\n",
        "        final_frontend = fut_frontend.result()\n",
        "        final_backend  = fut_backend.result()\n",
        "\n",
        "    return final_frontend, final_backend"
      ],
      "metadata": {
        "id": "lpT1UB2yLzGj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block Summary**\n",
        "\n",
        "This cell defines the **Formatter Agent** and its runner helper:\n",
        "\n",
        "- **`formatter_agent`**  \n",
        "  - An LLM agent tasked with assembling all pieces into one Markdown document  \n",
        "  - Has the role “Documentation Formatter” and a clear goal to produce sections in order  \n",
        "\n",
        "- **`run_formatter_step`**  \n",
        "  1. **Guard**: checks `scope_ok` before doing anything  \n",
        "  2. **Build Prompt**: embeds the optimized query, tech stack, API docs, frontend code, and backend code into a single instruction  \n",
        "  3. **Invoke** the `formatter_agent` via a one‑task `Crew`  \n",
        "  4. **Extract & Clean**: pulls out the raw Markdown and removes any accidental outer code fences  \n",
        "  5. **Return** the final Markdown string  \n",
        "\n",
        "With this, the system produces a cohesive document containing Overview, API Documentation, Frontend Code, and Backend Code sections.\n"
      ],
      "metadata": {
        "id": "vLn5I6f0_hCW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from crewai import Agent, Task, Crew\n",
        "\n",
        "# 1) Define the Formatter Agent\n",
        "formatter_agent = Agent(\n",
        "    role             = \"Documentation Formatter\",  # Agent’s title\n",
        "    goal             = (                             # What it must achieve\n",
        "        \"Assemble the optimized query, tech stack, API docs, frontend code, \"\n",
        "        \"and backend code into one coherent Markdown document.\"\n",
        "    ),\n",
        "    backstory        = \"Detail‑oriented technical writer and developer.\",\n",
        "    allow_delegation = False,                       # Don’t split into sub‑agents\n",
        "    llm              = \"gpt-4o-mini\"                # Model to use\n",
        ")\n",
        "\n",
        "def run_formatter_step(\n",
        "    opt_out: dict,\n",
        "    api_docs_md: str,\n",
        "    frontend_code: str,\n",
        "    backend_code: str\n",
        ") -> str:\n",
        "    \"\"\"\n",
        "    Takes optimizer output plus API docs, frontend, and backend code,\n",
        "    and returns a single Markdown document with the correct sections.\n",
        "    \"\"\"\n",
        "    # Guard: only proceed if the original query was in‑scope\n",
        "    if not opt_out.get(\"scope_ok\", False):\n",
        "        return \"⚠️ Out of scope. Nothing to format.\"\n",
        "\n",
        "    # Build the instruction for the agent, injecting all pieces\n",
        "    prompt = f\"\"\"\n",
        "You are the Documentation Formatter. Using the pieces below, produce **only**\n",
        "a Markdown document with these top‑level headings in this order:\n",
        "\n",
        "# Overview\n",
        "# API Documentation\n",
        "# Frontend Code\n",
        "# Backend Code\n",
        "\n",
        "### Overview\n",
        "**Task:** {opt_out['optimized_query']}\n",
        "**Tech Stack:**\n",
        "- Frontend: {opt_out['tech_stack']['frontend']}\n",
        "- Backend: {opt_out['tech_stack']['backend']}\n",
        "\n",
        "### API Documentation\n",
        "{api_docs_md}\n",
        "\n",
        "### Frontend Code\n",
        "```javascript\n",
        "{frontend_code}\n",
        "```\n",
        "### Backend Code\n",
        "```javascript\n",
        "{backend_code}\n",
        "```\n",
        "Return only the Markdown—no extra commentary. \"\"\".strip()\n",
        "\n",
        "    # Create the formatting task\n",
        "    task = Task(\n",
        "        agent           = formatter_agent,\n",
        "        description     = prompt,\n",
        "        expected_output = \"A single Markdown document with the specified sections.\"\n",
        "    )\n",
        "    # Run the agent\n",
        "    crew_output = Crew(agents=[formatter_agent], tasks=[task], verbose=False).kickoff()\n",
        "\n",
        "    # Extract the raw Markdown response\n",
        "    md = crew_output.tasks_output[0].raw\n",
        "\n",
        "    # If the LLM wrapped everything in ``` fences, remove them\n",
        "    if md.startswith(\"```\"):\n",
        "        md = \"\\n\".join(line for line in md.splitlines() if not re.match(r\"^```\", line)).strip()\n",
        "\n",
        "    return md\n"
      ],
      "metadata": {
        "id": "AOjIMfm4PO0J"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block Summary**\n",
        "\n",
        "This cell ties together the entire Sikka checkout AI pipeline and provides a CLI‑style entry point:\n",
        "\n",
        "- **`orchestrate(user_message)`**  \n",
        "  1. **Optimize** the user’s request into a structured plan (`opt_out`).  \n",
        "  2. **Generate API docs** based on that plan, showing only the first 300 characters as a snippet.  \n",
        "  3. **Produce & auto‑fix** both frontend and backend code in parallel, printing short snippets.  \n",
        "  4. **Format** all pieces into one cohesive Markdown document (`final_md`).  \n",
        "  5. **Return** the final Markdown (or an out‑of‑scope notice).\n",
        "\n",
        "- **Debug prints** at each stage help you trace progress in Colab or your terminal.\n",
        "\n",
        "- **`if __name__ == \"__main__\"`** block demonstrates how to call `orchestrate` directly when running the script, printing the complete output.\n",
        "\n",
        "With this cell, you can run the full end‑to‑end flow in one call and see live debug output for each step.\n"
      ],
      "metadata": {
        "id": "ldTXJg3m__e7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def orchestrate(user_message: str) -> str:\n",
        "    \"\"\"\n",
        "    Runs the full Sikka checkout pipeline:\n",
        "      1) Optimize the user query\n",
        "      2) Generate API docs\n",
        "      3) Generate & auto‑fix frontend + backend code\n",
        "      4) Format everything into Markdown\n",
        "\n",
        "    Returns the final Markdown (or an out‑of‑scope notice).\n",
        "    \"\"\"\n",
        "    # 1) Optimize: parse and structure the user’s request\n",
        "    opt_out = run_optimizer(user_message)\n",
        "    print(\"🔍 Optimizer Output:\", opt_out, \"\\n\" + \"─\" * 60, flush=True)\n",
        "    # If out of scope, immediately return notice\n",
        "    if not opt_out.get(\"scope_ok\", False):\n",
        "        return f\"⚠️ Out of scope: {opt_out.get('reason','')}\"\n",
        "\n",
        "    # 2) API docs: generate documentation and show a snippet\n",
        "    api_docs = run_api_docs_step(opt_out)\n",
        "    print(\"📄 API Documentation (snippet):\", api_docs[:300], \"\\n\" + \"─\" * 60, flush=True)\n",
        "\n",
        "    # 3) Code gen & refinement: run both pipelines in parallel\n",
        "    frontend_code, backend_code = run_frontend_and_backend(opt_out, api_docs)\n",
        "    print(\"🚀 Frontend Code (snippet):\", frontend_code[:200], flush=True)\n",
        "    print(\"🔧 Backend  Code (snippet):\", backend_code[:200], \"\\n\" + \"─\" * 60, flush=True)\n",
        "\n",
        "    # 4) Formatting: assemble all parts into final Markdown\n",
        "    final_md = run_formatter_step(opt_out, api_docs, frontend_code, backend_code)\n",
        "    print(\"🎉 Final Markdown generated.\", flush=True)\n",
        "\n",
        "    # Return the complete Markdown document\n",
        "    return final_md\n",
        "\n",
        "# ── Example usage ───────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    # Sample user request to kick off the pipeline\n",
        "    message = (\n",
        "        \"I want a React checkout page that saves a patient's card \"\n",
        "        \"and runs a $25 sale using Sikka sandbox.\"\n",
        "    )\n",
        "    # Run orchestration and print the result\n",
        "    doc = orchestrate(message)\n",
        "    print(\"\\n=== Final Output ===\\n\")\n",
        "    print(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-UrpwDkgL1Kx",
        "outputId": "8dad3b7a-4497-4877-92df-b72cbd68d03b"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🔍 Optimizer Output: {'scope_ok': True, 'optimized_query': \"Create a React checkout page that saves a patient's card and processes a $25 sale using Sikka's sandbox environment.\", 'tech_stack': {'frontend': 'React + JavaScript', 'backend': 'Node.js + Express + JavaScript'}, 'prompts': {'api_docs': \"Identify all required Sikka endpoints including the full base URL, path, and HTTP method. For card saving and processing a payment, you may need endpoints like: 1) POST https://api.sikkasoft.com/v4/payment/create, 2) POST https://api.sikkasoft.com/v4/card/save. The authentication flow involves obtaining a request_key on user authentication; ensure to include authorization headers such as {'Authorization': 'Bearer <token>'}. Sample request for card saving: { 'card_number': '4111111111111111', 'expiry_date': '12/25', 'cvv': '123' } Sample response: { 'success': true, 'card_id': '12345' }. Sample request for payment processing: { 'amount': 25, 'card_id': '12345' } Sample response: { 'success': true, 'transaction_id': 'abc123' }\", 'frontend': \"Build the checkout UI using React. Create components such as CheckoutForm.js for form handling, CardDetails.js for card input fields, and a Loader.js for loading indicators. Use React's useState for form state management and manage the API call with asynchronous functions, displaying error messages when applicable.\", 'backend': \"Implement server routes in Node.js and Express Framework, e.g., app.post('/api/save-card', saveCard) to handle card saving requests and app.post('/api/process-payment', processPayment) for payment processing. Include logic to obtain and refresh the request_key, e.g., by calling an authentication endpoint and storing the token. Call each identified Sikka endpoint using fetch or axios, handling JSON responses accordingly.\", 'formatter': 'Assemble all outputs into a single Markdown document. Include headings such as Overview to describe the purpose, API Documentation detailing endpoints, Frontend Code covering UI creation, and Backend Code showing server route implementations.'}} \n",
            "────────────────────────────────────────────────────────────\n",
            "📄 API Documentation (snippet): # Sikka API v2 Documentation\n",
            "\n",
            "## Overview\n",
            "The Sikka Payments API enables healthcare practices to easily process online payments through a secure payment gateway.\n",
            "\n",
            "## Base URL\n",
            "- Production: `https://api.sikkasoft.com/v2/payment`\n",
            "- Sandbox: `https://api.sikkasoft.com/v2/sandbox/payment`\n",
            "\n",
            "## Authentica \n",
            "────────────────────────────────────────────────────────────\n",
            "[DEBUG] Frontend eval_report: {'issues': [{'location': 'frontend', 'line': 38, 'message': 'Potential security risk: Authorization token is being exposed in client-side code.', 'severity': 'critical'}, {'location': 'frontend', 'line': 25, 'message': 'Improper error handling discloses system behavior. Avoid displaying generic error messages that could be exploited.', 'severity': 'critical'}, {'location': 'frontend', 'line': 42, 'message': 'Sending sensitive information via HTTP without SSL/TLS could lead to data interception.', 'severity': 'critical'}, {'location': 'frontend', 'line': 60, 'message': \"Input fields for sensitive information like card_number should use type='password' to prevent shoulder surfing.\", 'severity': 'critical'}], 'suggestions': ['Move the `requestKey` handling to a secure location, such as environment variables on the server.', 'Implement server-side validation and generic error messages to avoid exposing system details.', 'Ensure that all API calls are made over HTTPS to secure data transmissions.', \"Change the input type for sensitive information inputs to type='password' to increase user privacy.\"]}\n",
            "[DEBUG] Backend eval_report: {'issues': [{'location': 'backend', 'line': 39, 'message': 'Hardcoded sensitive information (username and password) is present in the code, leading to a security risk.', 'severity': 'critical'}, {'location': 'backend', 'line': 63, 'message': \"The variable 'YOUR_TOKEN' is not defined and will cause a runtime error when making an API call.\", 'severity': 'critical'}, {'location': 'backend', 'line': 75, 'message': 'The error response from the API is sent directly back to the client, which could expose sensitive information. It should be sanitized before sending.', 'severity': 'critical'}, {'location': 'backend', 'line': 89, 'message': 'Lack of input validation for incoming request data increases the risk for injection attacks.', 'severity': 'critical'}, {'location': 'backend', 'line': 49, 'message': 'No HTTPS enforcement or security headers are applied, risking transmitted data.', 'severity': 'critical'}], 'suggestions': ['Move sensitive credentials to environment variables and ensure they are not hardcoded in the source code.', \"Define and provide the 'YOUR_TOKEN' variable before its usage, possibly from an environment variable.\", 'Sanitize the API error responses by masking any sensitive information prior to sending back to the client.', 'Implement input validation and sanitization for incoming requests to prevent injection attacks.', 'Use HTTPS for API calls and set security headers to enhance the overall security of the backend.']}\n",
            "🚀 Frontend Code (snippet): // components/CheckoutForm.js\n",
            "import React, { useState } from 'react';\n",
            "import CardDetails from './CardDetails';\n",
            "import Loader from './Loader';\n",
            "\n",
            "const CheckoutForm = ({ patientInfo }) => {\n",
            "  const [car\n",
            "🔧 Backend  Code (snippet): // Import necessary modules\n",
            "const express = require('express');\n",
            "const axios = require('axios');\n",
            "const bodyParser = require('body-parser');\n",
            "const app = express();\n",
            "const PORT = process.env.PORT || 3000; \n",
            "────────────────────────────────────────────────────────────\n",
            "🎉 Final Markdown generated.\n",
            "\n",
            "=== Final Output ===\n",
            "\n",
            "# Overview\n",
            "**Task:** Create a React checkout page that saves a patient's card and processes a $25 sale using Sikka's sandbox environment.  \n",
            "**Tech Stack:**\n",
            "- Frontend: React + JavaScript\n",
            "- Backend: Node.js + Express + JavaScript\n",
            "\n",
            "# API Documentation\n",
            "# Sikka API v2 Documentation\n",
            "\n",
            "## Overview\n",
            "The Sikka Payments API enables healthcare practices to easily process online payments through a secure payment gateway.\n",
            "\n",
            "## Base URL\n",
            "- Production: `https://api.sikkasoft.com/v2/payment`\n",
            "- Sandbox: `https://api.sikkasoft.com/v2/sandbox/payment`\n",
            "\n",
            "## Authentication Flow\n",
            "To authenticate with the Sikka API, you need to obtain a **request_key** via user authentication. Follow these steps:\n",
            "1. Authenticate the user and receive a **request_key**.\n",
            "2. Use the request_key in your API calls as part of the request body or query parameters.\n",
            "3. Include the **Authorization** header in your requests:\n",
            "\n",
            "{\n",
            "    'Authorization': 'Bearer <token>'\n",
            "}\n",
            "\n",
            "## API Endpoints\n",
            "\n",
            "### 1. Save a Card\n",
            "- **Endpoint:** `POST https://api.sikkasoft.com/v2/payment/store_card`\n",
            "- **Headers:**\n",
            "    - `Content-Type`: `application/json`\n",
            "- **Request Body:**\n",
            "{\n",
            "    \"request_key\": \"request_key - mandatory\",\n",
            "    \"patient_id\": \"patient_id - mandatory\",\n",
            "    \"guarantor_id\": \"guarantor_id - mandatory\",\n",
            "    \"practice_id\": \"practice_id - mandatory\",\n",
            "    \"cust_id\": \"cust_id - mandatory\",\n",
            "    \"provider_id\": \"provider_id - optional\",\n",
            "    \"name\": \"name - mandatory\",\n",
            "    \"zipcode\": \"zipcode - mandatory\",\n",
            "    \"is_recurring_payments\": \"false - optional\",\n",
            "    \"card_number\": \"card_number - mandatory\",\n",
            "    \"expiration_month\": \"expiration_month(MM) - mandatory\",\n",
            "    \"expiration_year\": \"expiration_year(YYYY) - mandatory\",\n",
            "    \"address1\": \"address1 - optional\",\n",
            "    \"address2\": \"address2 - optional\",\n",
            "    \"city\": \"city - optional\",\n",
            "    \"state\": \"state - optional\",\n",
            "    \"phone\": \"phone - optional\",\n",
            "    \"email\": \"email - optional\",\n",
            "    \"is_default\": \"true/false - optional\"\n",
            "}\n",
            "- **Sample Response:**\n",
            "{\n",
            "    \"success\": true,\n",
            "    \"billing_id\": \"60EV88\"\n",
            "}\n",
            "\n",
            "### 2. Sale by Saved Card\n",
            "- **Endpoint:** `POST https://api.sikkasoft.com/v2/payment/sale`\n",
            "- **Headers:**\n",
            "    - `Content-Type`: `application/json`\n",
            "- **Request Body:**\n",
            "{\n",
            "    \"request_key\": \"request_key - mandatory\",\n",
            "    \"transaction_media\": \"billing_id - mandatory\",\n",
            "    \"patient_id\": \"patient_id - mandatory\",\n",
            "    \"guarantor_id\": \"guarantor_id - mandatory\",\n",
            "    \"practice_id\": \"practice_id - mandatory\",\n",
            "    \"cust_id\": \"cust_id - mandatory\",\n",
            "    \"provider_id\": \"provider_id - mandatory\",\n",
            "    \"name\": \"name - mandatory\",\n",
            "    \"zipcode\": \"zipcode - mandatory\",\n",
            "    \"transaction_amount\": \"transaction_amount(xx.xx) - mandatory\",\n",
            "    \"billing_id\": \"billing_id - mandatory\"\n",
            "}\n",
            "- **Sample Response:**\n",
            "{\n",
            "    \"success\": true,\n",
            "    \"transaction_id\": \"039-0012684312\"\n",
            "}\n",
            "\n",
            "### 3. Sale by Credit Card\n",
            "- **Endpoint:** `POST https://api.sikkasoft.com/v2/payment/sale`\n",
            "- **Headers:**\n",
            "    - `Content-Type`: `application/json`\n",
            "- **Request Body:**\n",
            "{\n",
            "    \"request_key\": \"request_key - mandatory\",\n",
            "    \"transaction_media\": \"credit_card - mandatory\",\n",
            "    \"patient_id\": \"patient_id - mandatory\",\n",
            "    \"guarantor_id\": \"guarantor_id - mandatory\",\n",
            "    \"practice_id\": \"practice_id - mandatory\",\n",
            "    \"cust_id\": \"cust_id - mandatory\",\n",
            "    \"provider_id\": \"provider_id - mandatory\",\n",
            "    \"transaction_amount\": \"transaction_amount(xx.xx) - mandatory\",\n",
            "    \"card_number\": \"card_number - mandatory\",\n",
            "    \"expiration_month\": \"expiration_month(MM) - mandatory\",\n",
            "    \"expiration_year\": \"expiration_year(YYYY) - mandatory\",\n",
            "    \"name\": \"name - mandatory\",\n",
            "    \"zipcode\": \"zipcode - mandatory\",\n",
            "    \"address1\": \"address1 - optional\",\n",
            "    \"address2\": \"address2 - optional\",\n",
            "    \"city\": \"city - optional\",\n",
            "    \"state\": \"state - optional\",\n",
            "    \"phone\": \"phone - optional\",\n",
            "    \"email\": \"email - optional\",\n",
            "    \"cvv\": \"cvv(XXX) - optional\"\n",
            "}\n",
            "- **Sample Response:**\n",
            "{\n",
            "    \"success\": true,\n",
            "    \"transaction_id\": \"039-0012681856\"\n",
            "}\n",
            "\n",
            "### 4. Retrieve Saved Card\n",
            "- **Endpoint:** `GET https://api.sikkasoft.com/v2/payment/store_card`\n",
            "- **Query Parameters:**\n",
            "    - `request_key`: *mandatory*\n",
            "    - `patient_id`: *mandatory*\n",
            "    - `guarantor_id`: *mandatory*\n",
            "    - `practice_id`: *mandatory*\n",
            "    - `startdate`: *optional* (format: yyyy-MM-dd HH:mm:ss)\n",
            "    - `enddate`: *optional* (format: yyyy-MM-dd HH:mm:ss)\n",
            "    - `billing_id`: *optional*\n",
            "    - `name`: *optional*\n",
            "    - `card_number`: *optional*\n",
            "    - `transaction_status`: *optional* (values: approved, accepted, declined, or all)\n",
            "    - `is_active`: *optional* (values: true, false, or all)\n",
            "    - `is_recurring_payments`: *optional* (values: true, false, or all)\n",
            "\n",
            "- **Sample Response:**\n",
            "{\n",
            "    \"success\": true,\n",
            "    \"cards\": [\n",
            "        {\n",
            "            \"billing_id\": \"60EV9H\",\n",
            "            \"card_number_masked\": \"1111\",\n",
            "            \"expiration_month\": \"04\",\n",
            "            \"expiration_year\": \"2029\",\n",
            "            \"is_active\": true,\n",
            "            \"name\": \"Mia Elson\",\n",
            "            \"zipcode\": \"95121\"\n",
            "        }\n",
            "    ]\n",
            "}\n",
            "\n",
            "## Important Notes\n",
            "- Ensure to comply with PCI regulations when handling credit card information.\n",
            "- Non-printable characters should not be submitted, and spaces within fields will be preserved.\n",
            "- Dates must be in the format \"YYYY-MM-DD\".\n",
            "\n",
            "This documentation provides essential endpoints and parameter details to implement the Sikka Payments API effectively. Ensure to handle responses properly and implement error handling for a robust integration.\n",
            "\n",
            "# Frontend Code\n",
            "// components/CheckoutForm.js\n",
            "import React, { useState } from 'react';\n",
            "import CardDetails from './CardDetails';\n",
            "import Loader from './Loader';\n",
            "\n",
            "const CheckoutForm = ({ patientInfo }) => {\n",
            "  const [cardData, setCardData] = useState({\n",
            "    name: '',\n",
            "    card_number: '',\n",
            "    expiration_month: '',\n",
            "    expiration_year: '',\n",
            "    zipcode: '',\n",
            "  });\n",
            "  const [loading, setLoading] = useState(false);\n",
            "  const [error, setError] = useState(null);\n",
            "  const [success, setSuccess] = useState(null);\n",
            "\n",
            "  const handleChange = (e) => {\n",
            "    const { name, value } = e.target;\n",
            "    setCardData((prevData) => ({ ...prevData, [name]: value }));\n",
            "  };\n",
            "\n",
            "  const handleSubmit = async (e) => {\n",
            "    e.preventDefault();\n",
            "    setLoading(true);\n",
            "    setError(null);\n",
            "    setSuccess(null);\n",
            "\n",
            "    try {\n",
            "      const response = await fetch('https://api.sikkasoft.com/v2/payment/store_card', {\n",
            "        method: 'POST',\n",
            "        headers: {\n",
            "          'Content-Type': 'application/json',\n",
            "          // Authorization token should not be handled on the client-side\n",
            "        },\n",
            "        body: JSON.stringify({\n",
            "          patient_id: patientInfo.patient_id,\n",
            "          guarantor_id: patientInfo.guarantor_id,\n",
            "          practice_id: patientInfo.practice_id,\n",
            "          cust_id: patientInfo.cust_id,\n",
            "          ...cardData,\n",
            "        }),\n",
            "      });\n",
            "\n",
            "      const result = await response.json();\n",
            "      if (result.success) {\n",
            "        setSuccess('Card saved successfully!');\n",
            "      } else {\n",
            "        setError('Failed to save card. Please try again.'); // Generic error message to prevent exploitation\n",
            "      }\n",
            "    } catch (err) {\n",
            "      setError('An error occurred. Please try again later.'); // Generic error message to prevent exploitation\n",
            "    } finally {\n",
            "      setLoading(false);\n",
            "    }\n",
            "  };\n",
            "\n",
            "  return (\n",
            "    <form onSubmit={handleSubmit}>\n",
            "      {loading && <Loader />}\n",
            "      {error && <div className=\"error\">{error}</div>}\n",
            "      {success && <div className=\"success\">{success}</div>}\n",
            "      \n",
            "      <CardDetails \n",
            "        cardData={cardData} \n",
            "        handleChange={handleChange} \n",
            "      />\n",
            "      <button type=\"submit\" disabled={loading}>\n",
            "        Save Card\n",
            "      </button>\n",
            "    </form>\n",
            "  );\n",
            "};\n",
            "\n",
            "export default CheckoutForm;\n",
            "\n",
            "// components/CardDetails.js\n",
            "import React from 'react';\n",
            "\n",
            "const CardDetails = ({ cardData, handleChange }) => {\n",
            "  return (\n",
            "    <div>\n",
            "      <input \n",
            "        type=\"text\" \n",
            "        name=\"name\" \n",
            "        value={cardData.name} \n",
            "        onChange={handleChange} \n",
            "        placeholder=\"Cardholder Name\" \n",
            "        required \n",
            "      />\n",
            "      <input \n",
            "        type=\"password\" // Changed to type='password' for card_number for privacy\n",
            "        name=\"card_number\" \n",
            "        value={cardData.card_number} \n",
            "        onChange={handleChange} \n",
            "        placeholder=\"Card Number\" \n",
            "        required \n",
            "      />\n",
            "      <input \n",
            "        type=\"text\" \n",
            "        name=\"expiration_month\" \n",
            "        value={cardData.expiration_month} \n",
            "        onChange={handleChange} \n",
            "        placeholder=\"MM\" \n",
            "        required \n",
            "      />\n",
            "      <input \n",
            "        type=\"text\" \n",
            "        name=\"expiration_year\" \n",
            "        value={cardData.expiration_year} \n",
            "        onChange={handleChange} \n",
            "        placeholder=\"YYYY\" \n",
            "        required \n",
            "      />\n",
            "      <input \n",
            "        type=\"text\" \n",
            "        name=\"zipcode\" \n",
            "        value={cardData.zipcode} \n",
            "        onChange={handleChange} \n",
            "        placeholder=\"Billing Zipcode\" \n",
            "        required \n",
            "      />\n",
            "    </div>\n",
            "  );\n",
            "};\n",
            "\n",
            "export default CardDetails;\n",
            "\n",
            "// components/Loader.js\n",
            "import React from 'react';\n",
            "\n",
            "const Loader = () => {\n",
            "  return <div className=\"loader\">Loading...</div>;\n",
            "};\n",
            "\n",
            "export default Loader;\n",
            "\n",
            "// styles.css (Add appropriate CSS for error and success feedback)\n",
            ".error {\n",
            "  color: red;\n",
            "}\n",
            "\n",
            ".success {\n",
            "  color: green;\n",
            "}\n",
            "\n",
            ".loader {\n",
            "  text-align: center;\n",
            "}\n",
            "\n",
            "# Backend Code\n",
            "// Import necessary modules\n",
            "const express = require('express');\n",
            "const axios = require('axios');\n",
            "const bodyParser = require('body-parser');\n",
            "const app = express();\n",
            "const PORT = process.env.PORT || 3000;\n",
            "const BASE_URL = 'https://api.sikkasoft.com/v2/payment'; // Change to sandbox URL for testing\n",
            "\n",
            "// Middleware\n",
            "app.use(bodyParser.json());\n",
            "\n",
            "// Environment Variables\n",
            "const USERNAME = process.env.YOUR_USERNAME;\n",
            "const PASSWORD = process.env.YOUR_PASSWORD;\n",
            "const YOUR_TOKEN = process.env.YOUR_TOKEN;\n",
            "\n",
            "// Function to get the request_key\n",
            "async function getRequestKey() {\n",
            "    const response = await axios.post(`${BASE_URL}/auth`, {\n",
            "        username: USERNAME,\n",
            "        password: PASSWORD\n",
            "    });\n",
            "\n",
            "    return response.data.request_key; // Assuming the response has request_key\n",
            "}\n",
            "\n",
            "// Sanitize error message\n",
            "function sanitizeError(error) {\n",
            "    return { message: error.response ? 'An error occurred' : 'Internal server error' };\n",
            "}\n",
            "\n",
            "// Route to save a card\n",
            "app.post('/api/save-card', async (req, res) => {\n",
            "    try {\n",
            "        // Validate input data\n",
            "        const { patient_id, guarantor_id, practice_id, cust_id, provider_id, name, zipcode, card_number, expiration_month, expiration_year } = req.body;\n",
            "        if (!patient_id || !card_number || !expiration_month || !expiration_year) {\n",
            "            return res.status(400).json({ message: 'Missing required fields' });\n",
            "        }\n",
            "\n",
            "        // Get request_key\n",
            "        const request_key = await getRequestKey();\n",
            "\n",
            "        // Prepare request payload\n",
            "        const payload = {\n",
            "            request_key,\n",
            "            patient_id,\n",
            "            guarantor_id,\n",
            "            practice_id,\n",
            "            cust_id,\n",
            "            provider_id,\n",
            "            name,\n",
            "            zipcode,\n",
            "            card_number,\n",
            "            expiration_month,\n",
            "            expiration_year\n",
            "        };\n",
            "\n",
            "        const response = await axios.post(`${BASE_URL}/store_card`, payload, {\n",
            "            headers: {\n",
            "                'Content-Type': 'application/json',\n",
            "                'Authorization': `Bearer ${YOUR_TOKEN}` // Assuming stored token\n",
            "            }\n",
            "        });\n",
            "\n",
            "        res.status(200).json(response.data);\n",
            "    } catch (error) {\n",
            "        console.error(error);\n",
            "        res.status(500).json(sanitizeError(error));\n",
            "    }\n",
            "});\n",
            "\n",
            "// Route to process payment\n",
            "app.post('/api/process-payment', async (req, res) => {\n",
            "    try {\n",
            "        // Validate input data\n",
            "        const { transaction_media, transaction_amount, patient_id, guarantor_id, practice_id, cust_id, provider_id, name, zipcode, billing_id } = req.body;\n",
            "        if (!transaction_media || !transaction_amount || !patient_id) {\n",
            "            return res.status(400).json({ message: 'Missing required fields' });\n",
            "        }\n",
            "\n",
            "        // Get request_key\n",
            "        const request_key = await getRequestKey();\n",
            "\n",
            "        // Prepare request payload\n",
            "        const payload = {\n",
            "            request_key,\n",
            "            transaction_media,\n",
            "            transaction_amount,\n",
            "            patient_id,\n",
            "            guarantor_id,\n",
            "            practice_id,\n",
            "            cust_id,\n",
            "            provider_id,\n",
            "            name,\n",
            "            zipcode,\n",
            "            billing_id\n",
            "        };\n",
            "\n",
            "        const response = await axios.post(`${BASE_URL}/sale`, payload, {\n",
            "            headers: {\n",
            "                'Content-Type': 'application/json',\n",
            "                'Authorization': `Bearer ${YOUR_TOKEN}` // Assuming stored token\n",
            "            }\n",
            "        });\n",
            "\n",
            "        res.status(200).json(response.data);\n",
            "    } catch (error) {\n",
            "        console.error(error);\n",
            "        res.status(500).json(sanitizeError(error));\n",
            "    }\n",
            "});\n",
            "\n",
            "// Start the server\n",
            "app.listen(PORT, () => {\n",
            "    console.log(`Server is running on port ${PORT}`);\n",
            "});\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Block Summary**\n",
        "\n",
        "This cell implements a simple chat interface with memory and context retrieval:\n",
        "\n",
        "- **`conversation` list**  \n",
        "  - Keeps full history of user and assistant messages for follow‑up continuity.\n",
        "\n",
        "- **`chat_with_memory(question: str) -> str`**  \n",
        "  1. Appends the new user question to `conversation`.  \n",
        "  2. Retrieves relevant API docs passages from FAISS via `retrieve_api_context`.  \n",
        "  3. Constructs `system_messages` including:  \n",
        "     - Instructions to use only provided docs.  \n",
        "     - The full final Markdown doc (`doc`).  \n",
        "     - The FAISS‑retrieved passages.  \n",
        "  4. Combines `system_messages` with the entire `conversation` history.  \n",
        "  5. Sends the batch to the LLM (`gpt-4o-mini`) and obtains the assistant reply.  \n",
        "  6. Appends and returns the assistant’s answer.\n",
        "\n",
        "- **Example usage**  \n",
        "  - Runs three follow‑up questions, printing each answer.  \n",
        "  - Finally pretty‑prints the full `conversation` history.\n",
        "\n",
        "This enables iterative Q&A over the generated documentation and code, maintaining context across turns.\n"
      ],
      "metadata": {
        "id": "q6y82W5OALUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conversation = []\n",
        "\n",
        "def chat_with_memory(question: str) -> str:\n",
        "    \"\"\"\n",
        "    Ask a follow‑up question, using both the final Markdown doc and\n",
        "    vector‑retrieved context. Maintains full conversation history.\n",
        "    \"\"\"\n",
        "    # 1) Add the new user message to the history\n",
        "    conversation.append({\"role\": \"user\", \"content\": question})\n",
        "\n",
        "    # 2) Retrieve relevant chunks from your FAISS index\n",
        "    vector_ctx = retrieve_api_context(question)\n",
        "\n",
        "    # 3) Build the system + history messages\n",
        "    system_messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": (\n",
        "                \"You are an expert on Sikka API integrations. \"\n",
        "                \"Use ONLY the documentation and relevant passages provided to answer.\"\n",
        "            )\n",
        "        },\n",
        "        {\"role\": \"system\", \"content\": f\"Full Documentation:\\n\\n{doc}\"},\n",
        "        {\"role\": \"system\", \"content\": f\"Relevant Passages:\\n\\n{vector_ctx}\"}\n",
        "    ]\n",
        "\n",
        "    # 4) Combine all messages: system directives + prior conversation\n",
        "    messages = system_messages + conversation\n",
        "\n",
        "    # 5) Query the model\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=messages\n",
        "    )\n",
        "    answer = response.choices[0].message.content\n",
        "\n",
        "    # 6) Append the assistant’s answer to the history and return it\n",
        "    conversation.append({\"role\": \"assistant\", \"content\": answer})\n",
        "    return answer\n",
        "\n",
        "# Example usage:\n",
        "q1 = \"What headers do I need for the payment endpoint?\"\n",
        "print(\"A1:\", chat_with_memory(q1))\n",
        "\n",
        "q2 = \"And how do I refresh the request_key in my server code?\"\n",
        "print(\"A2:\", chat_with_memory(q2))\n",
        "\n",
        "q3 = \"How should the UI handle rate‑limit errors?\"\n",
        "print(\"A3:\", chat_with_memory(q3))\n",
        "\n",
        "# Finally, inspect the conversation history:\n",
        "import pprint\n",
        "print(\"\\n=== Conversation History ===\")\n",
        "pprint.pprint(conversation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nIDp3DlVidme",
        "outputId": "65d8b8c8-2cb6-4202-d3c2-32049af4276d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A1: For the payment endpoints, you need to include the following headers:\n",
            "\n",
            "- `Content-Type`: `application/json`\n",
            "- `Authorization`: `Bearer <token>` \n",
            "\n",
            "The `Authorization` header requires a token, which should be obtained during the authentication process.\n",
            "A2: To refresh the `request_key` in your server code, you can make a POST request to the following endpoint with the required headers and body:\n",
            "\n",
            "### Endpoint:\n",
            "```\n",
            "POST https://api.sikkasoft.com/v4/request_key\n",
            "```\n",
            "\n",
            "### Headers:\n",
            "```\n",
            "Content-Type: application/json\n",
            "```\n",
            "\n",
            "### Request Body:\n",
            "You need to include the following JSON structure in the body:\n",
            "\n",
            "```json\n",
            "{\n",
            "    \"grant_type\": \"refresh_key\",\n",
            "    \"app_id\": \"app_id\",\n",
            "    \"app_key\": \"app_key\"\n",
            "}\n",
            "```\n",
            "\n",
            "In your server code, it would look something like this:\n",
            "\n",
            "```javascript\n",
            "const refreshRequestKey = async () => {\n",
            "    try {\n",
            "        const response = await axios.post('https://api.sikkasoft.com/v4/request_key', {\n",
            "            grant_type: \"refresh_key\",\n",
            "            app_id: YOUR_APP_ID,\n",
            "            app_key: YOUR_APP_KEY\n",
            "        }, {\n",
            "            headers: {\n",
            "                'Content-Type': 'application/json'\n",
            "            }\n",
            "        });\n",
            "        return response.data.request_key; // Use the new request_key received\n",
            "    } catch (error) {\n",
            "        console.error('Error refreshing request_key', error);\n",
            "        throw error; // Handle error appropriately\n",
            "    }\n",
            "};\n",
            "```\n",
            "\n",
            "This function will request a new `request_key`, which you can then use in subsequent API calls.\n",
            "A3: The UI should handle rate-limit errors by following these practices:\n",
            "\n",
            "1. **Display a User-Friendly Error Message**: When you receive an `HTTP 429 Too Many Requests` error, show a clear and friendly message to the user indicating that they have exceeded the request limit and to try again later. For example:\n",
            "   ```javascript\n",
            "   setError('You have exceeded the rate limit. Please wait a moment and try again.');\n",
            "   ```\n",
            "\n",
            "2. **Implement Retry Logic**: If applicable, implement a retry mechanism that waits for a specified amount of time before attempting the API call again. Use the `X-Rate-Limit-Reset` header to determine how long to wait before making another request.\n",
            "\n",
            "3. **Backoff Strategy**: Introduce an exponential backoff strategy where subsequent retries are spaced out more as repeated `HTTP 429` responses are received. This prevents hammering the API in case of continuous failures.\n",
            "\n",
            "4. **Disable Action Buttons**: Temporarily disable any action buttons (e.g., \"Submit\" or \"Save\") that trigger API calls until the rate limit is reset. This can help prevent further requests while waiting for the limit to increase.\n",
            "\n",
            "5. **User Notifications**: Optionally, you could show a countdown timer indicating when the user can try their request again based on the `X-Rate-Limit-Reset` value.\n",
            "\n",
            "6. **Log Errors**: Log the error details for monitoring purposes. This can help in diagnosing if users frequently face rate limit issues.\n",
            "\n",
            "By implementing these practices, you can create a more user-friendly experience that addresses the limitations imposed by the rate limits on the Sikka API.\n",
            "\n",
            "=== Conversation History ===\n",
            "[{'content': 'What headers do I need for the payment endpoint?',\n",
            "  'role': 'user'},\n",
            " {'content': 'For the payment endpoints, you need to include the following '\n",
            "             'headers:\\n'\n",
            "             '\\n'\n",
            "             '- `Content-Type`: `application/json`\\n'\n",
            "             '- `Authorization`: `Bearer <token>` \\n'\n",
            "             '\\n'\n",
            "             'The `Authorization` header requires a token, which should be '\n",
            "             'obtained during the authentication process.',\n",
            "  'role': 'assistant'},\n",
            " {'content': 'And how do I refresh the request_key in my server code?',\n",
            "  'role': 'user'},\n",
            " {'content': 'To refresh the `request_key` in your server code, you can make a '\n",
            "             'POST request to the following endpoint with the required headers '\n",
            "             'and body:\\n'\n",
            "             '\\n'\n",
            "             '### Endpoint:\\n'\n",
            "             '```\\n'\n",
            "             'POST https://api.sikkasoft.com/v4/request_key\\n'\n",
            "             '```\\n'\n",
            "             '\\n'\n",
            "             '### Headers:\\n'\n",
            "             '```\\n'\n",
            "             'Content-Type: application/json\\n'\n",
            "             '```\\n'\n",
            "             '\\n'\n",
            "             '### Request Body:\\n'\n",
            "             'You need to include the following JSON structure in the body:\\n'\n",
            "             '\\n'\n",
            "             '```json\\n'\n",
            "             '{\\n'\n",
            "             '    \"grant_type\": \"refresh_key\",\\n'\n",
            "             '    \"app_id\": \"app_id\",\\n'\n",
            "             '    \"app_key\": \"app_key\"\\n'\n",
            "             '}\\n'\n",
            "             '```\\n'\n",
            "             '\\n'\n",
            "             'In your server code, it would look something like this:\\n'\n",
            "             '\\n'\n",
            "             '```javascript\\n'\n",
            "             'const refreshRequestKey = async () => {\\n'\n",
            "             '    try {\\n'\n",
            "             '        const response = await '\n",
            "             \"axios.post('https://api.sikkasoft.com/v4/request_key', {\\n\"\n",
            "             '            grant_type: \"refresh_key\",\\n'\n",
            "             '            app_id: YOUR_APP_ID,\\n'\n",
            "             '            app_key: YOUR_APP_KEY\\n'\n",
            "             '        }, {\\n'\n",
            "             '            headers: {\\n'\n",
            "             \"                'Content-Type': 'application/json'\\n\"\n",
            "             '            }\\n'\n",
            "             '        });\\n'\n",
            "             '        return response.data.request_key; // Use the new '\n",
            "             'request_key received\\n'\n",
            "             '    } catch (error) {\\n'\n",
            "             \"        console.error('Error refreshing request_key', error);\\n\"\n",
            "             '        throw error; // Handle error appropriately\\n'\n",
            "             '    }\\n'\n",
            "             '};\\n'\n",
            "             '```\\n'\n",
            "             '\\n'\n",
            "             'This function will request a new `request_key`, which you can '\n",
            "             'then use in subsequent API calls.',\n",
            "  'role': 'assistant'},\n",
            " {'content': 'How should the UI handle rate‑limit errors?', 'role': 'user'},\n",
            " {'content': 'The UI should handle rate-limit errors by following these '\n",
            "             'practices:\\n'\n",
            "             '\\n'\n",
            "             '1. **Display a User-Friendly Error Message**: When you receive '\n",
            "             'an `HTTP 429 Too Many Requests` error, show a clear and friendly '\n",
            "             'message to the user indicating that they have exceeded the '\n",
            "             'request limit and to try again later. For example:\\n'\n",
            "             '   ```javascript\\n'\n",
            "             \"   setError('You have exceeded the rate limit. Please wait a \"\n",
            "             \"moment and try again.');\\n\"\n",
            "             '   ```\\n'\n",
            "             '\\n'\n",
            "             '2. **Implement Retry Logic**: If applicable, implement a retry '\n",
            "             'mechanism that waits for a specified amount of time before '\n",
            "             'attempting the API call again. Use the `X-Rate-Limit-Reset` '\n",
            "             'header to determine how long to wait before making another '\n",
            "             'request.\\n'\n",
            "             '\\n'\n",
            "             '3. **Backoff Strategy**: Introduce an exponential backoff '\n",
            "             'strategy where subsequent retries are spaced out more as '\n",
            "             'repeated `HTTP 429` responses are received. This prevents '\n",
            "             'hammering the API in case of continuous failures.\\n'\n",
            "             '\\n'\n",
            "             '4. **Disable Action Buttons**: Temporarily disable any action '\n",
            "             'buttons (e.g., \"Submit\" or \"Save\") that trigger API calls until '\n",
            "             'the rate limit is reset. This can help prevent further requests '\n",
            "             'while waiting for the limit to increase.\\n'\n",
            "             '\\n'\n",
            "             '5. **User Notifications**: Optionally, you could show a '\n",
            "             'countdown timer indicating when the user can try their request '\n",
            "             'again based on the `X-Rate-Limit-Reset` value.\\n'\n",
            "             '\\n'\n",
            "             '6. **Log Errors**: Log the error details for monitoring '\n",
            "             'purposes. This can help in diagnosing if users frequently face '\n",
            "             'rate limit issues.\\n'\n",
            "             '\\n'\n",
            "             'By implementing these practices, you can create a more '\n",
            "             'user-friendly experience that addresses the limitations imposed '\n",
            "             'by the rate limits on the Sikka API.',\n",
            "  'role': 'assistant'}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Next Steps\n",
        "\n",
        "- **Extract core pipeline functions**  \n",
        "  Consolidate `run_optimizer`, `run_api_docs_step`, `run_frontend_step`, `run_backend_step`, `run_formatter_step`, and `chat_with_memory` into a standalone module (e.g. `ai_pipeline.py`).\n",
        "\n",
        "- **Build a backend endpoint**  \n",
        "  Create an Express route that accepts user messages, invokes the AI pipeline functions, and returns JSON or Markdown responses.\n",
        "\n",
        "- **Develop a React chatbot UI**  \n",
        "  Scaffold a simple React app with a chat interface that sends user input to your backend and displays streaming or batched AI responses.\n",
        "\n",
        "- **Centralize configuration & secrets**  \n",
        "  Move API keys, model names, and other settings into a `.env` (or equivalent) and load via `dotenv` or environment variables.\n",
        "\n",
        "- **Add testing**  \n",
        "  Write unit tests that mock the AI calls, plus integration tests that spin up your backend and verify end‑to‑end behavior.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5B6HUJ2cmEYw"
      }
    }
  ]
}